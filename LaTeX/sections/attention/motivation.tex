\section{Motivation for Attention Mechanisms}
\begin{frame}{Motivation}
    \begin{itemize}
        \setlength{\itemsep}{0.5em}
        \item Sequence-to-sequence models struggle with \textbf{long-range dependencies}, making it difficult to capture context over lengthy inputs.
        \item Attention mechanisms address this by allowing the model to \textbf{selectively focus} on relevant parts of the input sequence during processing.
        \item They form the \textbf{backbone} of modern architectures such as \textbf{Transformer models} (e.g., BERT, GPT, ViT).
        \item Attention is crucial in various tasks including \textbf{translation, summarization, and image captioning}.
    \end{itemize}
    \textbf{Example:} In machine translation, rather than encoding an entire sentence into a single fixed-size vector, attention enables the model to dynamically focus on relevant source words when generating each target word.
\end{frame}