\section{Summary}
\begin{frame}{}
    \LARGE Summary
\end{frame}

\begin{frame}{Future Directions}
    \begin{itemize}
        \item \textbf{Contextual embeddings} (ELMo, BERT, GPT): Word vectors depend on sentence context.
        \item \textbf{Multilingual embeddings} and cross-lingual models.
        \item \textbf{Graph-based embeddings} (e.g., knowledge graph completion).
        \item \textbf{Hybrid embeddings}: Combining structured and unstructured data.
    \end{itemize}
\end{frame}

\begin{frame}{Key Takeaways}
    \begin{itemize}
        \item Vector Space Models (VSMs) are foundational for modern NLP.
        \item Word embeddings capture semantic relationships in a continuous space.
        \item Word2Vec, GloVe, and fastText are key methods for generating word vectors.
        \item Dense embeddings outperform one-hot vectors in capturing meaning and relationships.
        \item Future work focuses on contextual, multilingual, and hybrid embeddings.
    \end{itemize}
\end{frame}