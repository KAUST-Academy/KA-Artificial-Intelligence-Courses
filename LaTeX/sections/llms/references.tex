\section{References}
\begin{frame}[allowframebreaks]{References}
\begin{thebibliography}{10}
\bibitem{kaplan2020scaling}
Kaplan et al., ``Scaling Laws for Neural Language Models'', 2020.

\bibitem{devlin2018bert}
Devlin et al., ``BERT: Pre-training of Deep Bidirectional Transformers'', 2018.

\bibitem{brown2020gpt3}
Brown et al., ``Language Models are Few-Shot Learners (GPT-3)'', 2020.

\bibitem{xue2022byt5}
Xue et al., ``ByT5: Towards a token-free future with pre-trained byte-to-byte models'', 2022.

\bibitem{beltagy2020longformer}
Beltagy et al., ``Longformer: The Long-Document Transformer'', 2020.

\bibitem{tay2020efficient}
Tay et al., ``Efficient Transformers: A Survey'', 2020.

\bibitem{openai2023gpt4}
OpenAI, ``Technical Report on GPT-4'', 2023.

\bibitem{google_scaling}
Google Research Blog: Scaling Transformer Models.

\bibitem{anthropic2024claude}
Anthropic, ``Claude 3.5 Release Notes'', 2024.

\bibitem{flashattention2022}
FlashAttention: \url{https://arxiv.org/abs/2205.14135}
\end{thebibliography}
\end{frame}