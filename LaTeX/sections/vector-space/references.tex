\section{References}
\begin{frame}{}
    \LARGE References
\end{frame}

\begin{frame}[allowframebreaks]{References}
    \begin{thebibliography}{8}
        \bibitem{mikolov2013}
        Mikolov, T., Chen, K., Corrado, G., \& Dean, J. (2013).\\
        Efficient Estimation of Word Representations in Vector Space.\\
        \textit{arXiv:1301.3781}.

        \bibitem{pennington2014}
        Pennington, J., Socher, R., \& Manning, C. D. (2014).\\
        GloVe: Global Vectors for Word Representation.\\
        \textit{EMNLP}.

        \bibitem{bojanowski2017}
        Bojanowski, P., Grave, E., Joulin, A., \& Mikolov, T. (2017).\\
        Enriching Word Vectors with Subword Information.\\
        \textit{TACL}.

        \bibitem{jurafsky2023}
        Jurafsky, D., \& Martin, J. H. (2023).\\
        Speech and Language Processing (3rd Ed Draft).

        \bibitem{stanfordnlp}
        Stanford NLP Slides.\\
        \url{https://web.stanford.edu/class/cs224n}

        \bibitem{fasttext}
        Facebook AI Research (FAIR) – fastText.\\
        \url{https://fasttext.cc}

        \bibitem{goldberg2016}
        Goldberg, Y. (2016).\\
        A Primer on Neural Network Models for NLP.\\
        \textit{JMLR}.

        \bibitem{chrupala2019}
        Chrupała, G. (2019).\\
        Symbolic and Subsymbolic Representations in NLP – Deep Learning Lectures.
    \end{thebibliography}
\end{frame}