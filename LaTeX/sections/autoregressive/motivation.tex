% \begin{frame}[allowframebreaks]{Autoregressive Models: Motivation}
% \begin{itemize}
%     \item \textbf{Motivation:}
%     \begin{itemize}
%         \item How can we model and generate complex data distributions?
%         \item What are the challenges in moving from simple to high-dimensional data?
%     \end{itemize}
%     \item \textbf{1-Dimensional Distributions:}
%     \begin{itemize}
%         \item \textit{Simplest generative model:} Histogram-based modeling.
%         \item \textit{Parameterized distributions:} Fit data with known distributions (e.g., Gaussian, Bernoulli).
%         \item \textit{Maximum likelihood:} Learn parameters by maximizing the likelihood of observed data.
%     \end{itemize}
%     \item \textbf{High-Dimensional Distributions:}
%     \begin{itemize}
%         \item Curse of dimensionality: Direct modeling becomes infeasible.
%         \item \textit{Chain rule:} Factorize joint distribution as a product of conditionals:
%         \[
%             p(x_1, x_2, \ldots, x_n) = \prod_{i=1}^n p(x_i \mid x_{<i})
%         \]
%     \end{itemize}
%     \item \textbf{“Practical” Incarnations:}
%     \begin{itemize}
%         \item \textit{Bayesian Networks:} Graphical models encoding conditional dependencies.
%         \item \textit{MADE:} Masked Autoencoder for Distribution Estimation.
%         \item \textit{Causal Masked Neural Models:} Neural networks with causal masking (e.g., PixelCNN, Transformer decoders).
%         \item \textit{RNNs:} Recurrent Neural Networks for sequential data modeling.
%     \end{itemize}
%     \item \textbf{Deeper Dive into Causal Masked Neural Models:}
%     \begin{itemize}
%         \item \textit{Convolutional:} PixelCNN, WaveNet use masked convolutions for autoregressive modeling.
%         \item \textit{Attention:} Transformers with causal masks for flexible context modeling.
%         \item \textit{Tokenization:} Discretizing data (e.g., bytes, words, image patches) for sequence modeling.
%         \item \textit{Caching:} Efficient inference and generation by reusing computed states.
%     \end{itemize}
%     \item \textbf{Other Considerations:}
%     \begin{itemize}
%         \item \textit{Decoder-only vs. Encoder-Decoder:} Trade-offs in architecture for generation and conditioning.
%         \item \textit{New incarnations of recurrent models:} Modern RNN variants and hybrids.
%         \item \textit{Alternative/complementary tokenization:} Exploring new ways to represent and process data sequences.
%     \end{itemize}
% \end{itemize}

% \framebreak

% \begin{itemize}
%     \item \textbf{Problems we’d like to solve:}
%     \begin{itemize}
%         \item \textit{Generate data:} Synthesize images, videos, speech, and text.
%         \item \textit{Compress data:} Construct efficient codes for storage and transmission.
%         \item \textit{Detect anomalies:} Identify data that is out of distribution.
%     \end{itemize}
%     \item \textbf{Likelihood-based models:}
%     \begin{itemize}
%         \item Estimate the data distribution $p_{\text{data}}$ from samples $x^{(1)}, \ldots, x^{(n)} \sim p_{\text{data}}(x)$.
%         \item Learn a distribution $p$ that enables:
%         \begin{itemize}
%             \item Computing $p(x)$ for arbitrary $x$.
%             \item Sampling $x \sim p(x)$.
%         \end{itemize}
%     \end{itemize}
%     \item \textbf{Focus for today:} Discrete data.
% \end{itemize}

% \framebreak

% \begin{itemize}
%     \item \textbf{Why model the joint distribution of data?}
%     \begin{itemize}
%         \item To understand, generate, and manipulate complex data such as text, images, and audio.
%         \item Enables probabilistic reasoning and sampling from the data distribution.
%     \end{itemize}
%     \item \textbf{Goal:}
%     \begin{itemize}
%         \item Build tractable probabilistic models with explicit likelihoods.
%         \item Facilitate tasks like generation, completion, and denoising.
%     \end{itemize}
%     \item \textbf{Examples:}
%     \begin{itemize}
%         \item \textbf{Text generation:} Language models (e.g., GPT, RNN-based models)
%         \item \textbf{Image generation:} PixelCNN, PixelRNN
%         \item \textbf{Audio synthesis:} WaveNet
%     \end{itemize}
% \end{itemize}
% \end{frame}

\begin{frame}[allowframebreaks]{}
    \begin{figure}
        \centering
        \fetchconvertimage{https://www.researchgate.net/profile/Nanxin-Chen/publication/355206261/figure/fig1/AS:1078866160422912@1634233145910/llustrations-of-autoregressive-and-non-autoregressive-ASR.png}{images/autoregressive/architecture-asr.png}{width=\textwidth,keepaspectratio}
        \caption*{Illustrations of autoregressive and non-autoregressive ASR.}
    \end{figure}
\end{frame}

\begin{frame}[allowframebreaks]{Motivation}
    \textbf{The Core Problem:} \\
    How do we handle really big, complicated data?
    \begin{itemize}
        \item \textit{Example:} A single 128x128 color image has nearly 50,000 numbers (pixels) to describe it!
        \item Trying to model all these numbers together is way too hard—there are just too many possibilities.
        \item This is called the "curse of dimensionality": as data gets bigger, the problem gets much harder.
    \end{itemize}

    \framebreak

    
    \textbf{Desired Properties for our Models:}
    \begin{itemize}
        \item \textbf{Computational Efficiency:}
        \begin{itemize}
            \item Efficient training (fast convergence, reasonable resource usage).
            \item Efficient model representation (compact size).
        \end{itemize}
        \item \textbf{Statistical Efficiency:}
        \begin{itemize}
            \item Expressiveness (ability to capture complex data relationships).
            \item Generalization (performing well on unseen data from the same distribution).
        \end{itemize}
        \item \textbf{Performance Metrics:}
        \begin{itemize}
            \item High sampling quality and speed (for generating new data).
            \item Good compression rate and speed (for data storage and transmission).
        \end{itemize}
    \end{itemize}
    
\end{frame}

