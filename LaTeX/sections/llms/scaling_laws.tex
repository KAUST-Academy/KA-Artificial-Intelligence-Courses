\section{Scaling Laws for LLMs}
\begin{frame}{Scaling Laws for LLMs}
    \begin{itemize}
        \item \textbf{Kaplan et al. (2020):} ``Scaling Laws for Neural Language Models''
        \item Performance improves predictably with:
        \begin{itemize}
            \item More parameters
            \item More compute
            \item Larger datasets
        \end{itemize}
        \item \textbf{Optimal allocation of compute:} Train bigger models with less data, rather than small models with lots of data.
        \item \textbf{Implication:} LLMs like GPT-3 (175B), GPT-4 (est. $>$500B) are products of scaling laws.
    \end{itemize}
\end{frame}