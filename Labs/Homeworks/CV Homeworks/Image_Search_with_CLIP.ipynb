{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Image Search Engine with CLIP Embeddings - Homework Assignment\n",
    "\n",
    "![CLIP Architecture](https://github.com/openai/CLIP/raw/main/CLIP.png)\n",
    "\n",
    "In this homework, you will implement an **Image Search Engine** using CLIP (Contrastive Language-Image Pre-training) embeddings on the Tiny-ImageNet validation dataset. CLIP allows you to search for images using both text queries and image queries.\n",
    "\n",
    "## üìå Project Overview\n",
    "- **Task**: Build a multimodal image search engine\n",
    "- **Architecture**: Pre-trained CLIP model for feature extraction\n",
    "- **Dataset**: Tiny-ImageNet validation set\n",
    "- **Goal**: Retrieve most similar images given text or image queries\n",
    "\n",
    "## üìö Learning Objectives\n",
    "By completing this assignment, you will:\n",
    "- Understand multimodal embeddings and their applications\n",
    "- Learn to use pre-trained CLIP models for feature extraction\n",
    "- Implement similarity search using cosine similarity\n",
    "- Evaluate zero-shot classification performance\n",
    "- Build a practical image retrieval system"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1Ô∏è‚É£ Dataset Setup\n",
    "\n",
    "**Task**: Download and explore the Tiny-ImageNet validation dataset.\n",
    "\n",
    "**Requirements**:\n",
    "- Install the tinyimagenet package\n",
    "- Load the validation split of the dataset\n",
    "- Explore the dataset structure and class labels\n",
    "- Visualize sample images from different classes"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Install tinyimagenet package (uncomment the line below)\n",
    "# !pip install tinyimagenet\n",
    "\n",
    "# TODO: Import necessary libraries\n",
    "\n",
    "# TODO: Load the validation dataset\n",
    "\n",
    "\n",
    "# TODO: Print dataset information\n",
    "\n",
    "\n",
    "# TODO: Display 5 sample images with their class information"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2Ô∏è‚É£ Import Libraries and Configuration\n",
    "\n",
    "**Task**: Import all necessary libraries and set up the environment for CLIP.\n",
    "\n",
    "**Requirements**:\n",
    "- Import PyTorch, transformers, and other necessary libraries\n",
    "- Load the pre-trained CLIP model and processor\n",
    "- Set up device configuration (GPU if available)\n",
    "- Configure any necessary parameters"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Import all necessary libraries\n",
    "\n",
    "\n",
    "# TODO: Check device availability\n",
    "\n",
    "\n",
    "# TODO: Load pre-trained CLIP model and processor\n",
    "\n",
    "\n",
    "# TODO: Move model to device\n",
    "\n",
    "\n",
    "# TODO: Print model information"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3Ô∏è‚É£ Feature Extraction from Dataset\n",
    "\n",
    "**Task**: Extract CLIP embeddings for all images in the validation dataset.\n",
    "\n",
    "**Requirements**:\n",
    "- Process all validation images through CLIP\n",
    "- Extract and store image embeddings\n",
    "- Normalize embeddings for cosine similarity computation\n",
    "- Save embeddings for efficient searching"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Create function to extract image embeddings\n",
    "\n",
    "#\n",
    "#         # TODO: Collect batch of images\n",
    "\n",
    "#         # TODO: Process batch through CLIP\n",
    "\n",
    "#\n",
    "#         # TODO: Print progress\n",
    "\n",
    "#     # TODO: Concatenate all embeddings\n",
    "\n",
    "\n",
    "# TODO: Extract embeddings for the validation dataset\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4Ô∏è‚É£ Zero-Shot Classification Evaluation\n",
    "\n",
    "**Task**: Evaluate CLIP's zero-shot classification performance on Tiny-ImageNet.\n",
    "\n",
    "**Requirements**:\n",
    "- Create text prompts for each image in Tiny-ImageNet\n",
    "    - Check the dataset words.txt!!!!\n",
    "- Extract text embeddings for class descriptions\n",
    "- Perform zero-shot classification using similarity matching\n",
    "- Calculate and report accuracy metrics"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Create text prompts for all classes\n",
    "\n",
    "\n",
    "# TODO: Extract text embeddings for class prompts\n",
    "\n",
    "\n",
    "# TODO: Perform zero-shot classification\n",
    "\n",
    "# TODO: Calculate similarities and predictions\n",
    "\n",
    "# TODO: Calculate accuracy\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5Ô∏è‚É£ Image Search Engine Implementation\n",
    "\n",
    "**Task**: Build functions to search for similar images using both text and image queries.\n",
    "\n",
    "**Requirements**:\n",
    "- Implement text-to-image search functionality\n",
    "- Implement image-to-image search functionality\n",
    "- Return top-k most similar images\n",
    "- Create visualization functions for search results"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Implement text-to-image search\n",
    "\n",
    "\n",
    "# TODO: Implement image-to-image search\n",
    "\n",
    "\n",
    "# TODO: Create visualization function\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6Ô∏è‚É£ Testing with Custom Queries\n",
    "\n",
    "**Task**: Test your search engine with custom text queries and web images.\n",
    "\n",
    "**Requirements**:\n",
    "- Test with 5 different text queries\n",
    "- Download and test with 5 images from the web\n",
    "- Display top 5 most similar images for each query\n",
    "- Analyze the quality of retrieved results"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Test with text queries",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Test with image queries from web\n",
    "\n",
    "\n",
    "# TODO: Define image URLs for testing\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üìù Evaluation Criteria\n",
    "\n",
    "Your homework will be evaluated based on:\n",
    "\n",
    "1. **Implementation Correctness (40%)**\n",
    "   - Proper CLIP model loading and usage\n",
    "   - Correct feature extraction for images and text\n",
    "   - Working search functionality for both text and image queries\n",
    "   - Accurate zero-shot classification implementation\n",
    "\n",
    "2. **Search Results Quality (30%)**\n",
    "   - Reasonable search results for text queries\n",
    "   - Appropriate image-to-image search results\n",
    "   - Correct similarity calculations and ranking\n",
    "   - Zero-shot classification accuracy\n",
    "\n",
    "3. **Code Quality (20%)**\n",
    "   - Clean, readable code with proper comments\n",
    "   - Efficient implementation with batch processing\n",
    "   - Proper error handling and edge cases\n",
    "   - Well-structured functions\n",
    "\n",
    "4. **Testing and Demonstration (10%)**\n",
    "   - Successful testing with custom queries\n",
    "   - Clear visualization of search results\n",
    "   - Proper documentation of testing process"
   ]
  }
 ]
}
