\section{Summary}
\begin{frame}{}
    \LARGE N-gram Models: \textbf{Summary}
\end{frame}

\begin{frame}{Beyond N-grams}
    \begin{itemize}
        \item Neural Language Models (e.g., Word2Vec, LSTMs)
        \item Transformer-based Models (BERT, GPT)
        \item Subword Tokenization (Byte-Pair Encoding)
        \item Pretrained Language Models
        \item Contextual Representations
    \end{itemize}
\end{frame}

\begin{frame}{Summary}
    \begin{itemize}
        \item NLP enables machines to understand human language
        \item N-grams model word sequences simply and effectively
        \item Sequence notation and probability estimation are essential
        \item Start/end/UNK tokens improve modeling and robustness
        \item N-gram models are limited $\rightarrow$ neural models offer solutions
    \end{itemize}
\end{frame}