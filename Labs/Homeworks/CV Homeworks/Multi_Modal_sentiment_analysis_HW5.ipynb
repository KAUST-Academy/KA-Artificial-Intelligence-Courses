{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyO6D4Do3UoQrHQasGRGvwzl"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image.png](https://i.imgur.com/a3uAqnb.png)\n"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multi-Modal Sentiment Analysis - Homework Assignment\n",
    "\n",
    "![Combined Model Architecture](https://i.imgur.com/RVYyBe7.jpeg)\n",
    "\n",
    "In this homework, you will build and compare three different models for sentiment analysis on a dataset of tweets, each containing both an image and text.\n",
    "\n",
    "## üìå Project Overview\n",
    "- **Task**: Classify the sentiment of a tweet (positive, negative, or neutral) using its text, its image, and a combination of both.\n",
    "- **Architecture**:\n",
    "    1. An image-only model (CNN).\n",
    "    2. A text-only model (RNN/LSTM/GRU).\n",
    "    3. A combined, multi-modal model that fuses features from the two.\n",
    "- **Dataset**: MVSA (Multi-view Social Data)\n",
    "- **Goal**: Compare the performance of unimodal vs. multi-modal approaches for sentiment analysis.\n",
    "\n",
    "## üìö Learning Objectives\n",
    "By completing this assignment, you will:\n",
    "- Understand how to process a mixed-media dataset (images and text).\n",
    "- Implement a CNN for image classification.\n",
    "- Build an RNN/LSTM for text classification.\n",
    "- Construct a multi-modal architecture by combining feature extractors.\n",
    "- Evaluate and compare the performance of different models on the same task."
   ],
   "metadata": {
    "id": "mDwrM_Td-iwB"
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1Ô∏è‚É£ Dataset Setup (PROVIDED)\n",
    "\n",
    "The MVSA dataset has been downloaded for you. The dataset structure is as follows:\n",
    "- `labelResultAll.txt`: A file containing the labels for each data point. The format is `tweet_id,label`.\n",
    "- `data/`: A folder containing all the image (`.jpg`) and text (`.txt`) files, named by their tweet ID."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T9sfwiYP0w7H",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1720777860678,
     "user_tz": -60,
     "elapsed": 2601,
     "user": {
      "displayName": "Muhammad Mubashar",
      "userId": "14515435323579848862"
     }
    },
    "outputId": "5a53cbde-80a5-41f0-f6bb-9d19c37bed07",
    "ExecuteTime": {
     "end_time": "2025-07-11T22:13:49.525628Z",
     "start_time": "2025-07-11T22:13:49.149560Z"
    }
   },
   "source": [
    "import kagglehub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Dataset already downloaded and prepared\n",
    "load_dotenv()\n",
    "path = kagglehub.dataset_download(\"vincemarcs/mvsasingle\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Let's check the contents\n",
    "print(\"\\nContents of MVSA_Single:\")\n",
    "print(os.listdir(os.path.join(path, 'MVSA_Single')))\n",
    "\n",
    "print(\"\\nSample of files in the data folder:\")\n",
    "print(os.listdir(os.path.join(path, 'MVSA_Single', 'data'))[:5])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ali/.cache/kagglehub/datasets/vincemarcs/mvsasingle/versions/1\n",
      "\n",
      "Contents of MVSA_Single:\n",
      "['data', 'labelResultAll.txt']\n",
      "\n",
      "Sample of files in the data folder:\n",
      "['3033.jpg', '1951.txt', '1304.txt', '3070.jpg', '1149.txt']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2Ô∏è‚É£ Import Libraries and Configuration\n",
    "\n",
    "**Task**: Import all necessary libraries and set up configuration parameters.\n",
    "\n",
    "**Requirements**:\n",
    "- Import PyTorch, torchvision, pandas, and other utilities.\n",
    "- Import libraries for text processing and evaluation (e.g., NLTK, Scikit-learn).\n",
    "- Set random seeds for reproducibility.\n",
    "- Configure hyperparameters with reasonable values."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#TODO: Import all necessary libraries (torch, nn, optim, pandas, etc.)\n",
    "#TODO: Set random seeds for reproducibility (use seed=42)\n",
    "#TODO: Check device availability and print (e.g., \"cuda\" or \"cpu\")\n",
    "#TODO: Define configuration parameters:\n",
    "IMG_SIZE = 224 # Image size (e.g., for ResNet)\n",
    "BATCH_SIZE = 32 # Batch size\n",
    "LEARNING_RATE = 1e-4 # Learning rate\n",
    "NUM_EPOCHS = 10 # Number of training epochs\n",
    "VOCAB_SIZE = 10000 # Maximum vocabulary size for text\n",
    "MAX_LEN = 50 # Max sequence length for text"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3Ô∏è‚É£ Data Loading and Preprocessing\n",
    "\n",
    "**Task**: Load the labels, match them with their corresponding image and text files, and split the data.\n",
    "\n",
    "**Requirements**:\n",
    "- Read `labelResultAll.txt` into a pandas DataFrame.\n",
    "- Map labels from ('positive', 'negative', 'neutral') to (0, 1, 2).\n",
    "- Create a list of all data samples, where each sample is a tuple `(image_path, text_path, label)`.\n",
    "- Split this list into training and validation sets (e.g., 80:20 split)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:15:02.786303Z",
     "start_time": "2025-07-11T22:15:02.782840Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Construct the full path to the data and label files\n",
    "\n",
    "\n",
    "# TODO: Read 'labelResultAll.txt' using pandas. The file has no header and is comma-separated.\n",
    "# Name the columns ['id', 'label'].\n",
    "\n",
    "\n",
    "# TODO: Convert string labels ('positive', 'negative', 'neutral') to integer labels (0, 1, 2).\n",
    "# You can use a dictionary for mapping.\n",
    "\n",
    "\n",
    "# TODO: Create a list of all data samples. Each item should be a tuple:\n",
    "# (path_to_image, path_to_text, integer_label)\n",
    "# Iterate through the DataFrame and create the file paths.\n",
    "\n",
    "\n",
    "# TODO: Split the data into training and validation sets using train_test_split from scikit-learn.\n",
    "# Use a test_size of 0.2 and a random_state of 42.\n",
    "\n",
    "\n",
    "# TODO: Print the number of samples in the training and validation sets."
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4Ô∏è‚É£ Text and Image Transformations\n",
    "\n",
    "**Task**: Define the preprocessing pipelines for both images and text.\n",
    "\n",
    "**Requirements**:\n",
    "- For images: Define transforms to resize, convert to tensor, and normalize.\n",
    "- For text:\n",
    "    - Build a vocabulary from the training text data.\n",
    "    - Create a text pipeline function to tokenize, numericalize (convert tokens to integers), and pad sequences."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:15:16.360992Z",
     "start_time": "2025-07-11T22:15:16.358114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Define image transforms using transforms.Compose:\n",
    "#       - Resize to (IMG_SIZE, IMG_SIZE)\n",
    "#       - ToTensor()\n",
    "#       - Normalize with mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] (ImageNet stats)\n",
    "\n",
    "\n",
    "# TODO: Build the text vocabulary.\n",
    "#       - Create a tokenizer function (e.g., using NLTK or basic string split).\n",
    "#       - Iterate through the text files in your *training data*.\n",
    "#       - Build a vocabulary that maps words to indices. Include special tokens for padding ('<pad>') and unknown words ('<unk>').\n",
    "\n",
    "\n",
    "# TODO: Define a text processing pipeline function.\n",
    "#       This function should take a text file path, read the text, tokenize it,\n",
    "#       convert tokens to their corresponding vocabulary indices, and pad/truncate the sequence to MAX_LEN."
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5Ô∏è‚É£ Custom Dataset and DataLoaders\n",
    "\n",
    "**Task**: Create a custom Dataset class and set up DataLoaders.\n",
    "\n",
    "**Requirements**:\n",
    "- The `Dataset` should handle loading one sample (image, text, and label).\n",
    "- `__getitem__` should apply the image transforms and text processing pipeline.\n",
    "- Create `DataLoader` instances for both training and validation sets."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:15:32.963153Z",
     "start_time": "2025-07-11T22:15:32.960237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Create a MVSADataset class inheriting from torch.utils.data.Dataset.\n",
    "# TODO: In __init__(self, data_samples, image_transform, text_pipeline):\n",
    "#       - Store the samples list, transforms, and text pipeline.\n",
    "# TODO: Implement __len__ to return the number of samples.\n",
    "# TODO: Implement __getitem__ to:\n",
    "#       - Get the image_path, text_path, and label for the given index.\n",
    "#       - Load the image with PIL, apply the image_transform.\n",
    "#       - Apply the text_pipeline to the text_path to get a processed tensor.\n",
    "#       - Return (processed_image, processed_text, label).\n",
    "\n",
    "# TODO: Create train_dataset and val_dataset using your MVSADataset class.\n",
    "\n",
    "# TODO: Create train_loader and val_loader with DataLoader.\n",
    "#       - Use appropriate BATCH_SIZE and shuffle for the training loader."
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6Ô∏è‚É£ Part A: Image-Only Model (CNN)\n",
    "\n",
    "First, we will build, train, and evaluate a model that only uses the images to predict sentiment.\n",
    "\n",
    "### 6.1 Define the CNN Architecture\n",
    "**Task**: Create a CNN for image classification. A pre-trained model like ResNet is a good choice.\n",
    "\n",
    "**Requirements**:\n",
    "- Load a pre-trained ResNet (e.g., ResNet18).\n",
    "- Replace the final fully connected layer to match the number of sentiment classes (3)."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:15:50.050893Z",
     "start_time": "2025-07-11T22:15:50.047464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Define the ImageModel class or function.\n",
    "#       - Use models.resnet18(pretrained=True).\n",
    "#       - Freeze the parameters of the pre-trained layers to avoid updating them initially.\n",
    "#       - Replace the final `fc` layer with a new `nn.Linear` layer with 3 output units.\n",
    "\n",
    "# TODO: Initialize the image model and move it to the configured device.\n",
    "# TODO: Print the model architecture."
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6.2 Train and Evaluate the Image Model\n",
    "**Task**: Write the training and evaluation loop for the image-only model.\n",
    "\n",
    "**Requirements**:\n",
    "- Set up the loss function (CrossEntropyLoss) and optimizer (Adam).\n",
    "- Loop through epochs and batches to train the model.\n",
    "- After training, evaluate the model's performance on the validation set.\n",
    "- Calculate and display the final accuracy and a confusion matrix."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:16:12.405455Z",
     "start_time": "2025-07-11T22:16:12.402656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Instantiate the loss function (nn.CrossEntropyLoss).\n",
    "# TODO: Instantiate the optimizer (e.g., optim.Adam) for the image model's parameters.\n",
    "\n",
    "# TODO: Write the training loop for NUM_EPOCHS:\n",
    "#       For each batch in train_loader:\n",
    "#       - Get images and labels, move them to the device.\n",
    "#       - Zero gradients.\n",
    "#       - Get model outputs.\n",
    "#       - Calculate loss.\n",
    "#       - Backpropagate and update weights.\n",
    "\n",
    "# TODO: Write the evaluation loop:\n",
    "#       - Set model to evaluation mode (model.eval()).\n",
    "#       - Use torch.no_grad() to disable gradient calculations.\n",
    "#       - Iterate through the val_loader, get predictions.\n",
    "#       - Collect all true labels and predictions.\n",
    "\n",
    "# TODO: Calculate and print the final validation accuracy.\n",
    "# TODO: Generate and plot a confusion matrix for the image model."
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 7Ô∏è‚É£ Part B: Text-Only Model (RNN/LSTM)\n",
    "\n",
    "Next, we will build a model that uses only the text from the tweets.\n",
    "\n",
    "### 7.1 Define the RNN/LSTM Architecture\n",
    "**Task**: Create a text classification model using an Embedding layer and an LSTM/GRU layer.\n",
    "\n",
    "**Requirements**:\n",
    "- An `nn.Embedding` layer to convert word indices to dense vectors.\n",
    "- An `nn.LSTM` or `nn.GRU` layer to process the sequence.\n",
    "- A final `nn.Linear` layer to produce class scores."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:16:21.443810Z",
     "start_time": "2025-07-11T22:16:21.440828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Define the TextModel class inheriting from nn.Module.\n",
    "#       In __init__:\n",
    "#       - Create an nn.Embedding layer (vocab_size, embedding_dim).\n",
    "#       - Create an nn.LSTM or nn.GRU layer.\n",
    "#       - Create a nn.Linear layer for the output classification (hidden_dim -> 3).\n",
    "#       In forward(self, text):\n",
    "#       - Pass text through embedding layer.\n",
    "#       - Pass embeddings through LSTM/GRU.\n",
    "#       - Use the final hidden state of the LSTM/GRU for classification.\n",
    "#       - Pass the hidden state through the linear layer.\n",
    "\n",
    "# TODO: Initialize the text model and move it to the device.\n",
    "# TODO: Print the model architecture."
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7.2 Train and Evaluate the Text Model\n",
    "**Task**: Train and evaluate the text-only model using the same process as before."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:16:38.900023Z",
     "start_time": "2025-07-11T22:16:38.897425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Instantiate the loss function and optimizer for the text model.\n",
    "\n",
    "# TODO: Write the training loop for the text model.\n",
    "#       For each batch in train_loader:\n",
    "#       - Get texts and labels, move them to the device.\n",
    "#       - Train the model (forward pass, loss, backward pass, optimizer step).\n",
    "\n",
    "# TODO: Write the evaluation loop for the text model on the validation set.\n",
    "\n",
    "# TODO: Calculate and print the final validation accuracy.\n",
    "# TODO: Generate and plot a confusion matrix for the text model."
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 8Ô∏è‚É£ Part C: Combined Multimodal Model\n",
    "\n",
    "Finally, we'll combine the two feature extractors into a single, powerful model.\n",
    "\n",
    "### 8.1 Define the Multimodal Architecture\n",
    "**Task**: Create a model that takes both an image and text as input.\n",
    "\n",
    "**Requirements**:\n",
    "- Use the pre-trained image CNN (without its final classifier layer) as an image feature extractor.\n",
    "- Use the trained text model (without its final classifier layer) as a text feature extractor.\n",
    "- Concatenate the features from both branches.\n",
    "- Add one or more `nn.Linear` layers to classify the combined feature vector."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:16:49.855697Z",
     "start_time": "2025-07-11T22:16:49.852900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Define the MultiModalModel class inheriting from nn.Module.\n",
    "#       In __init__:\n",
    "#       - Instantiate your image feature extractor (e.g., ResNet without the last layer).\n",
    "#       - Instantiate your text feature extractor (e.g., Embedding + LSTM).\n",
    "#       - Define a new classifier (nn.Sequential with Linear, ReLU, Dropout, Linear)\n",
    "#         that takes the concatenated feature dimension as input.\n",
    "#       In forward(self, image, text):\n",
    "#       - Get image features.\n",
    "#       - Get text features.\n",
    "#       - Concatenate the features (torch.cat).\n",
    "#       - Pass the combined features through the new classifier.\n",
    "#       - Return the final logits.\n",
    "\n",
    "# TODO: Initialize the multimodal model and move it to the device.\n",
    "# TODO: Print the model architecture."
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8.2 Train and Evaluate the Multimodal Model\n",
    "**Task**: Train and evaluate the final combined model."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:17:14.909401Z",
     "start_time": "2025-07-11T22:17:14.906626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Instantiate the loss function and optimizer for the multimodal model.\n",
    "\n",
    "# TODO: Write the training loop for the multimodal model.\n",
    "#       For each batch in train_loader:\n",
    "#       - Get images, texts, and labels, move them to the device.\n",
    "#       - Train the model (forward pass, loss, backward pass, optimizer step).\n",
    "\n",
    "# TODO: Write the evaluation loop for the multimodal model on the validation set.\n",
    "\n",
    "# TODO: Calculate and print the final validation accuracy.\n",
    "# TODO: Generate and plot a confusion matrix for the multimodal model."
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 9Ô∏è‚É£ Performance Comparison\n",
    "\n",
    "**Task**: Present the results of all three models side-by-side.\n",
    "\n",
    "**Requirements**:\n",
    "- Display the final validation accuracies for the Image-Only, Text-Only, and Multimodal models.\n",
    "- Plot the confusion matrices for all three models in a single figure for easy comparison."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-11T22:17:35.559482Z",
     "start_time": "2025-07-11T22:17:35.556327Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Print the final validation accuracies for all three models in a summary table or list.\n",
    "\n",
    "# TODO: Create a 1x3 subplot using matplotlib.\n",
    "# TODO: Plot the confusion matrix for the image model in the first subplot.\n",
    "# TODO: Plot the confusion matrix for the text model in the second subplot.\n",
    "# TODO: Plot the confusion matrix for the multimodal model in the third subplot.\n",
    "# TODO: Add titles to each subplot.\n",
    "# TODO: Display the final plot."
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üìù Evaluation Criteria\n",
    "\n",
    "Your homework will be evaluated based on:\n",
    "\n",
    "1.  **Implementation Correctness (40%)**\n",
    "    - Correct implementation of all three model architectures (CNN, RNN/LSTM, Combined).\n",
    "    - Proper data loading, preprocessing, and splitting.\n",
    "    - Working training and evaluation loops for each model.\n",
    "\n",
    "2.  **Model Training and Results (30%)**\n",
    "    - All three models train without errors.\n",
    "    - Loss decreases over epochs.\n",
    "    - Final models produce reasonable predictions on the validation set.\n",
    "\n",
    "3.  **Code Quality (20%)**\n",
    "    - Clean, readable code with comments explaining key parts.\n",
    "    - Correct use of PyTorch modules, tensor shapes, and data flow.\n",
    "    - Efficient implementation.\n",
    "\n",
    "4.  **Comparison and Visualization (10%)**\n",
    "    - Clear presentation of final accuracies for all models.\n",
    "    - Correctly generated and clearly labeled confusion matrices for comparison."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Written by: Ali Habibullah"
  }
 ]
}
