\begin{frame}{Limitations and Considerations of RNNs}
    \begin{itemize}
        \item \textbf{Long-term Dependencies:} \\
        Basic Recurrent Neural Networks (RNNs) have difficulty capturing dependencies in sequences longer than approximately 10 steps. This limitation arises due to issues like vanishing and exploding gradients, which hinder the learning of long-range relationships in data.
        
        \item \textbf{Computational Cost:} \\
        Training RNNs is generally slower compared to more modern architectures such as transformers. This is because RNNs process sequences sequentially, making it challenging to leverage parallel computation effectively.
        
        \item \textbf{Alternatives:} \\
        Transformer models and Large Language Models (LLMs) have become popular alternatives. They offer superior parallel processing capabilities and are more effective at modeling long-range dependencies in sequential data.
    \end{itemize}
\end{frame}