\section{Policy Gradients}
\begin{frame}{}
    \LARGE Reinforcement Learning: \textbf{Policy Gradients}
\end{frame}

\begin{frame}{Policy Gradients}
    \begin{itemize}
        \setlength{\itemsep}{1em}
        \item {\large \textbf{What is the problem with Q-learning?}}
        \item The Q-function can be very complex.
        \item For example, a robot grasping an object may have a very high-dimensional state space. It can be difficult to learn the exact Q-value for every (state, action) pair.
        \pause
        \item However, the policy itself can be much simpler; for instance, just closing the robot's hand.
        \item Can we learn a policy directly, i.e., find the best policy from a set of possible policies?
    \end{itemize}
\end{frame}

\begin{frame}{Policy Gradients}
    \begin{itemize}
        \item Formally, let us define a class of parameterized policies: 
        $$\Pi = \{ \pi_\theta \mid \theta \in \mathbb{R}^m \}$$
        \item For each policy, we can define its expected return:
        $$\mathcal{J}(\theta) = \mathbb{E} \left [ \sum_{t \geq 0} \gamma^t r_t \mid \pi_\theta \right ]$$
        \pause
        \item Our goal is to find the optimal policy: $\theta^{\star} = \arg \max_\theta \mathcal{J}(\theta)$
        \item How can we achieve this?
        \pause
        \item \textbf{Solution}: Perform gradient ascent on the policy parameters!
    \end{itemize}
\end{frame}

\begin{frame}{Policy Gradients}
    \begin{figure}
        \centering
        \fetchconvertimage{https://miro.medium.com/v2/resize:fit:1400/1*94EI9DpoXnWa6oLHvh14pw.jpeg}{images/policygrad+reinforce+actor/policy_gradient.png}{width=\textwidth,height=0.9\textheight,keepaspectratio}
    \end{figure}
\end{frame}
