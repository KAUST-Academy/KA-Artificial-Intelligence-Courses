{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![image.png](https://i.imgur.com/a3uAqnb.png)\n",
   "id": "18058b5f5c46ddc3"
  },
  {
   "cell_type": "markdown",
   "id": "b53b4dac",
   "metadata": {},
   "source": [
    "# Text-to-Image Generation using Stable Diffusion - Homework Assignment\n",
    "\n",
    "![Stable Diffusion Architecture](https://miro.medium.com/v2/resize:fit:1400/1*NpQ282NJdOfxUsYlwLJplA.png)\n",
    "\n",
    "In this homework, you will finetune a **Stable Diffusion** model to generate Naruto-style images from text descriptions. This involves working with the complete diffusion pipeline including VAE, UNet, text encoder, and scheduler.\n",
    "\n",
    "## üìå Project Overview\n",
    "- **Task**: Text-to-Naruto image generation\n",
    "- **Architecture**: Stable Diffusion with UNet diffusion model\n",
    "- **Dataset**: Naruto-style dataset with text descriptions\n",
    "- **Goal**: Generate realistic Naruto-style images from text prompts\n",
    "\n",
    "## üìö Learning Objectives\n",
    "By completing this assignment, you will:\n",
    "- Understand diffusion models and the stable diffusion pipeline\n",
    "- Learn to finetune pre-trained diffusion models\n",
    "- Work with VAE, UNet, text encoders, and schedulers\n",
    "- Practice text-to-image generation techniques\n",
    "- Handle memory constraints with large models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f022d71",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Dataset Setup (PROVIDED)\n",
    "\n",
    "The Naruto-style dataset has been loaded for you. The dataset contains:\n",
    "- 1,221 training images with corresponding text descriptions\n",
    "- Each sample has an 'image' and 'text' field\n",
    "- Images are in various sizes and need to be resized to 512x512\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3eeecc38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T00:57:23.121088Z",
     "start_time": "2025-07-12T00:57:22.525983Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Dataset already loaded\n",
    "ds = load_dataset(\"Alex-0402/naruto-style-dataset-with-text\")\n",
    "print(\"Dataset info:\", ds)\n",
    "print(\"Number of training samples:\", len(ds['train']))\n",
    "\n",
    "# Display a sample\n",
    "sample = ds['train'][0]\n",
    "print(\"\\nSample text:\", sample['text'])\n",
    "print(\"Image size:\", sample['image'].size)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'text'],\n",
      "        num_rows: 1221\n",
      "    })\n",
      "})\n",
      "Number of training samples: 1221\n",
      "\n",
      "Sample text: a man with dark hair and brown eyes, naruto style\n",
      "Image size: (1080, 1080)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "14dd0a00",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Import Libraries and Configuration\n",
    "\n",
    "**Task**: Import all necessary libraries and set up configuration parameters.\n",
    "\n",
    "**Requirements**:\n",
    "- Import diffusers, transformers, and related libraries\n",
    "- Import PyTorch, PIL, numpy, and other utilities\n",
    "- Set random seeds for reproducibility\n",
    "- Configure hyperparameters for stable diffusion training"
   ]
  },
  {
   "cell_type": "code",
   "id": "0b33c2f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T00:57:25.574341Z",
     "start_time": "2025-07-12T00:57:25.571156Z"
    }
   },
   "source": [
    "# TODO: Import all necessary libraries:\n",
    "#       - torch, torch.nn, torch.optim\n",
    "#       - diffusers (UNet2DConditionModel, AutoencoderKL, PNDMScheduler, etc.)\n",
    "#       - transformers (CLIPTextModel, CLIPTokenizer)\n",
    "#       - PIL, numpy, matplotlib\n",
    "#       - torchvision.transforms\n",
    "#       - tqdm for progress bars\n",
    "\n",
    "# TODO: Set random seeds for reproducibility (use seed=42)\n",
    "\n",
    "# TODO: Check device availability and print\n",
    "\n",
    "# TODO: Define configuration parameters:\n",
    "MODEL_ID = \"OFA-Sys/small-stable-diffusion-v0\"  # Smaller stable diffusion model\n",
    "IMG_SIZE = 512  # Image resolution\n",
    "BATCH_SIZE = 1  # Small batch size for memory constraints\n",
    "LEARNING_RATE = 1e-5  # Learning rate for finetuning\n",
    "NUM_EPOCHS = 3  # Number of training epochs\n",
    "INFERENCE_STEPS = 100  # Number of denoising steps during inference\n",
    "GUIDANCE_SCALE = 7.5  # Classifier-free guidance scale"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ab2ff29e",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Load Pre-trained Stable Diffusion Components\n",
    "\n",
    "**Task**: Load all components of the stable diffusion pipeline.\n",
    "\n",
    "**Requirements**:\n",
    "- Load VAE (Variational Autoencoder) for image encoding/decoding\n",
    "- Load UNet for the diffusion process\n",
    "- Load text encoder and tokenizer for text conditioning\n",
    "- Load noise scheduler for the diffusion process\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c5dc5a7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T00:57:27.343498Z",
     "start_time": "2025-07-12T00:57:27.340484Z"
    }
   },
   "source": [
    "# TODO: Load stable diffusion components:\n",
    "#       - vae = AutoencoderKL.from_pretrained(MODEL_ID, subfolder=\"vae\")\n",
    "#       - unet = UNet2DConditionModel.from_pretrained(MODEL_ID, subfolder=\"unet\")\n",
    "#       - text_encoder = CLIPTextModel.from_pretrained(MODEL_ID, subfolder=\"text_encoder\")\n",
    "#       - tokenizer = CLIPTokenizer.from_pretrained(MODEL_ID, subfolder=\"tokenizer\")\n",
    "#       - scheduler = PNDMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
    "\n",
    "# TODO: Move models to device\n",
    "# TODO: Set VAE and text encoder to eval mode (only UNet will be trained)\n",
    "# TODO: Print model information and parameter counts"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "0483c282",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Preprocessing and Custom Dataset\n",
    "\n",
    "**Task**: Create custom dataset class and preprocessing pipeline.\n",
    "\n",
    "**Requirements**:\n",
    "- Resize images to 512x512 resolution\n",
    "- Normalize images to [-1, 1] range for VAE\n",
    "- Tokenize text descriptions\n",
    "- Handle data augmentation appropriately"
   ]
  },
  {
   "cell_type": "code",
   "id": "c08cf9ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T00:57:28.555920Z",
     "start_time": "2025-07-12T00:57:28.553477Z"
    }
   },
   "source": [
    "# TODO: Create NarutoDataset class inheriting from torch.utils.data.Dataset\n",
    "# TODO: In __init__(self, dataset, tokenizer, size=512):\n",
    "#       - Store dataset, tokenizer, and image size\n",
    "#       - Define image transforms:\n",
    "#         * Resize to (size, size)\n",
    "#         * Random horizontal flip for augmentation\n",
    "#         * ToTensor()\n",
    "#         * Normalize with mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]\n",
    "\n",
    "# TODO: Implement __len__ to return dataset length\n",
    "# TODO: Implement __getitem__ to:\n",
    "#       - Get image and text from dataset\n",
    "#       - Apply transforms to image\n",
    "#       - Tokenize text with padding and truncation\n",
    "#       - Return dict with 'pixel_values' and 'input_ids'\n",
    "\n",
    "# TODO: Create train_dataset and train_dataloader\n",
    "# TODO: Print dataset info and test with one sample"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "ce59c221",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Training Setup and Loss Function\n",
    "\n",
    "**Task**: Set up the training components including optimizer and loss function.\n",
    "\n",
    "**Requirements**:\n",
    "- Create optimizer for UNet parameters only\n",
    "- Implement the diffusion loss (noise prediction loss)\n",
    "- Set up proper gradient scaling and mixed precision if needed\n",
    "- Configure learning rate scheduling"
   ]
  },
  {
   "cell_type": "code",
   "id": "ae291bb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T00:57:29.677490Z",
     "start_time": "2025-07-12T00:57:29.674499Z"
    }
   },
   "source": [
    "# TODO: Create optimizer for UNet parameters only:\n",
    "#       - optimizer = torch.optim.AdamW(unet.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# TODO: Create noise scheduler for training (different from inference)\n",
    "#       - noise_scheduler = DDPMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
    "\n",
    "# TODO: Define helper functions:\n",
    "#       - encode_text(text_input): tokenize and encode text to embeddings\n",
    "#       - encode_image(image): encode image to latent space using VAE\n",
    "#       - decode_latent(latent): decode latent back to image using VAE\n",
    "\n",
    "# TODO: Set up gradient scaler for mixed precision training (optional)\n",
    "# TODO: Initialize training tracking variables"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "fc2589ce",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Training Loop Implementation\n",
    "\n",
    "**Task**: Implement the main training loop for diffusion model finetuning.\n",
    "\n",
    "**Requirements**:\n",
    "- Encode images to latent space using VAE\n",
    "- Add noise to latents according to diffusion schedule\n",
    "- Predict noise using UNet conditioned on text\n",
    "- Compute loss between predicted and actual noise\n",
    "- Update UNet parameters via backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "id": "060b2d28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T00:57:32.402958Z",
     "start_time": "2025-07-12T00:57:32.399004Z"
    }
   },
   "source": [
    "# TODO: Implement training loop:\n",
    "#       \n",
    "#       For each epoch:\n",
    "#         For each batch in train_dataloader:\n",
    "#           - Get images and text from batch\n",
    "#           - Encode images to latent space using VAE\n",
    "#           - Encode text to embeddings using text encoder\n",
    "#           - Sample random timesteps for diffusion\n",
    "#           - Add noise to latents according to schedule\n",
    "#           - Predict noise using UNet with text conditioning\n",
    "#           - Compute MSE loss between predicted and actual noise\n",
    "#           - Backpropagate and update UNet parameters\n",
    "#           - Track and display training progress\n",
    "#\n",
    "# TODO: Use torch.no_grad() for VAE and text encoder operations\n",
    "# TODO: Implement proper error handling and memory management\n",
    "# TODO: Save model checkpoints periodically\n",
    "# TODO: Display loss curves and training statistics"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "d398bffa",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Inference Pipeline Setup\n",
    "\n",
    "**Task**: Create inference pipeline for text-to-image generation.\n",
    "\n",
    "**Requirements**:\n",
    "- Set up complete diffusion pipeline with trained UNet\n",
    "- Configure scheduler for inference (100 steps)\n",
    "- Implement text-to-image generation function\n",
    "- Handle classifier-free guidance"
   ]
  },
  {
   "cell_type": "code",
   "id": "1f393036",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T00:57:33.065242Z",
     "start_time": "2025-07-12T00:57:33.062429Z"
    }
   },
   "source": [
    "# TODO: Create inference pipeline:\n",
    "#       - Set all models to eval mode\n",
    "#       - Create StableDiffusionPipeline with trained components\n",
    "#       - Configure scheduler for inference\n",
    "\n",
    "# TODO: Implement generate_image function that:\n",
    "#       - Takes text prompt as input\n",
    "#       - Encodes text to embeddings\n",
    "#       - Starts with random noise\n",
    "#       - Performs denoising for specified number of steps\n",
    "#       - Decodes final latent to image\n",
    "#       - Returns PIL image\n",
    "\n",
    "# TODO: Set up proper inference configuration:\n",
    "#       - num_inference_steps = INFERENCE_STEPS\n",
    "#       - guidance_scale = GUIDANCE_SCALE\n",
    "#       - Enable safety checker if desired"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "ce4bea33",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Generate Images with Dataset Prompts\n",
    "\n",
    "**Task**: Generate images using 5 prompts from the training dataset.\n",
    "\n",
    "**Requirements**:\n",
    "- Select 5 different text prompts from the dataset\n",
    "- Generate images for each prompt\n",
    "- Display results in a grid format\n",
    "- Show prompt text alongside generated images"
   ]
  },
  {
   "cell_type": "code",
   "id": "91741e9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T00:57:33.968629Z",
     "start_time": "2025-07-12T00:57:33.965270Z"
    }
   },
   "source": [
    "# TODO: Select 5 prompts from training dataset:\n",
    "#       - Use different indices to get variety\n",
    "#       - Extract text descriptions\n",
    "\n",
    "# TODO: Generate images for each dataset prompt:\n",
    "#       - Use generate_image function\n",
    "#       - Set random seed for reproducibility\n",
    "#       - Save generated images\n",
    "\n",
    "# TODO: Create visualization:\n",
    "#       - Display each prompt text\n",
    "#       - Show corresponding generated image\n",
    "#       - Use matplotlib subplot for clean layout\n",
    "#       - Add titles and proper formatting\n",
    "\n",
    "# TODO: Display results in a 2x3 grid or similar arrangement"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "8ffe12b7",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Generate Images with Custom Prompts\n",
    "\n",
    "**Task**: Generate images using 5 custom prompts that you create.\n",
    "\n",
    "**Requirements**:\n",
    "- Write 5 creative prompts in Naruto style\n",
    "- Test different types of descriptions (characters, scenes, actions)\n",
    "- Generate and display results\n",
    "- Compare quality with dataset prompt results"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5ac4c64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-12T00:57:35.097025Z",
     "start_time": "2025-07-12T00:57:35.094113Z"
    }
   },
   "source": [
    "# TODO: Define 5 custom prompts, for example:\n",
    "#       - \"A ninja with orange hair performing a jutsu\"\n",
    "#       - \"A village hidden in the leaves at sunset\"\n",
    "#       - \"A powerful chakra aura surrounding a young ninja\"\n",
    "#       - \"A battle scene with multiple ninjas using different techniques\"\n",
    "#       - \"A peaceful training ground with cherry blossoms\"\n",
    "\n",
    "# TODO: Generate images for each custom prompt:\n",
    "#       - Use same generation parameters as before\n",
    "#       - Ensure consistent quality\n",
    "\n",
    "# TODO: Create visualization for custom prompts:\n",
    "#       - Similar layout to dataset prompts\n",
    "#       - Show prompt text and generated image\n",
    "#       - Use consistent formatting\n",
    "\n",
    "# TODO: Display all 5 custom prompt results"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "f0c68346",
   "metadata": {},
   "source": [
    "## üîü Model Evaluation and Comparison\n",
    "\n",
    "**Task**: Evaluate and compare your results\n",
    "\n",
    "**Requirements**:\n",
    "- Compare generated images with original dataset images\n",
    "- Evaluate image quality, style consistency, and prompt adherence\n",
    "- Plot training progress and loss convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6976a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create comparison visualization:\n",
    "#       - Show original dataset images alongside generated ones\n",
    "#       - Compare style consistency\n",
    "#       - Evaluate prompt adherence\n",
    "\n",
    "# TODO: Plot training loss curve:\n",
    "#       - Show loss progression over epochs\n",
    "#       - Analyze convergence behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c8434",
   "metadata": {},
   "source": [
    "## üìù Evaluation Criteria\n",
    "\n",
    "Your homework will be evaluated based on:\n",
    "\n",
    "1. **Implementation Correctness (40%)**\n",
    "   - Proper stable diffusion pipeline setup\n",
    "   - Correct training loop implementation\n",
    "   - Working inference pipeline\n",
    "   - Appropriate use of VAE, UNet, text encoder, and scheduler\n",
    "\n",
    "2. **Training and Results (30%)**\n",
    "   - Model trains without errors\n",
    "   - Reasonable loss convergence\n",
    "   - Generated images show Naruto style characteristics\n",
    "   - Successful generation from both dataset and custom prompts\n",
    "\n",
    "3. **Code Quality (30%)**\n",
    "   - Clean, readable code with proper comments\n",
    "   - Efficient memory usage and error handling\n",
    "   - Proper tensor operations and device management\n",
    "   - Good visualization and presentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
