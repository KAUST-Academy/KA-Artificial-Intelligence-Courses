\begin{frame}{Motivation}
    \textbf{Why move beyond CNNs?}
    \begin{itemize}
        \item \textbf{Limitations of CNNs:} While CNNs excel at capturing local patterns, their strong inductive biases can restrict modeling of global context.
        \item \textbf{Vision Transformers (ViTs):} ViTs leverage self-attention mechanisms to effectively capture long-range dependencies across an image.
        \item \textbf{Performance and Flexibility:} On large-scale vision tasks, ViTs often match or surpass CNNs in accuracy, and offer greater architectural flexibility.
        \item \textbf{Broader Applicability:} The transformer framework enables unified modeling across different modalities, bridging vision and language tasks.
    \end{itemize}
\end{frame}