\section{Implementing CNNs in PyTorch}
\begin{frame}{}
    \LARGE CNN: \textbf{Implementing CNNs in PyTorch}
\end{frame}

\begin{frame}[allowframebreaks]{PyTorch Implementation of CNNs}
    \ttfamily\footnotesize{
    import torch \\
    import torch.nn as nn \\
    import torch.nn.functional as F \\

    class SimpleCNN(nn.Module): \\
    \hspace{1em}    def \_\_init\_\_(self): \\
    \hspace{2em}         super(SimpleCNN, self).\_\_init\_\_() \\
    \hspace{2em}        self.conv1 = nn.Conv2d(3, 32, kernel\_size=3, padding=1) \\
    \hspace{2em}        self.pool = nn.MaxPool2d(2, 2) \\
    \hspace{2em}        self.conv2 = nn.Conv2d(32, 64, kernel\_size=3, padding=1) \\
    \hspace{2em}        self.fc1 = nn.Linear(64 * 8 * 8, 128) \\
    \hspace{2em}        self.fc2 = nn.Linear(128, 10)  \# assuming 10 classes \\

    \hspace{1em}    def forward(self, x): \\
    \hspace{2em}        x = self.pool(F.relu(self.conv1(x))) \\
    \hspace{2em}        x = self.pool(F.relu(self.conv2(x))) \\
    \hspace{2em}        x = x.view(-1, 64 * 8 * 8) \\
    \hspace{2em}        x = F.relu(self.fc1(x)) \\
    \hspace{2em}        x = self.fc2(x) \\
    \hspace{2em}        return x
    }

\framebreak
    \ttfamily\footnotesize{
    import torch.optim as optim \\

    model = SimpleCNN() \\
    criterion = nn.CrossEntropyLoss() \\
    optimizer = optim.Adam(model.parameters(), lr=0.001) \\

    \# Training loop \\
    for epoch in range(10): \\
    \hspace{1em}    for images, labels in dataloader: \\
    \hspace{2em}        optimizer.zero\_grad() \\
    \hspace{2em}        outputs = model(images) \\
    \hspace{2em}        loss = criterion(outputs, labels) \\
    \hspace{2em}        loss.backward() \\
    \hspace{2em}        optimizer.step()
    }
\end{frame}