\begin{frame}{}
    \LARGE NLP: \textbf{References}
\end{frame}

\begin{frame}[allowframebreaks]{References}

\begin{thebibliography}{99}

\bibitem{jurafsky2023speech}
Jurafsky, D., & Martin, J. H. (2023).
\textit{Speech and Language Processing} (3rd ed.).
Draft chapters available at \url{https://web.stanford.edu/~jurafsky/slp3/}
% NLP Basics

\bibitem{mourri_kaiser_nlp_specialization}
Mourri, Y., & Kaiser, Ł.
Natural Language Processing Specialization.
DeepLearning.AI.
% NLP Course

\bibitem{raj_singh_cmu}
Raj, B., & Singh, R.
11-785 Introduction to Deep Learning.
Carnegie Mellon University (CMU).
% Deep Learning Course

\bibitem{elman1990finding}
Elman, J. L. (1990).
Finding structure in time.
\textit{Cognitive Science}, 14(2), 179-211.
% Recurrent Neural Networks (RNNs)

\bibitem{cho2014learning}
Cho, K., van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., & Bengio, Y. (2014).
Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation.
\textit{EMNLP 2014}.
% Gated Recurrent Unit (GRU)

\bibitem{hochreiter1997long}
Hochreiter, S., & Schmidhuber, J. (1997).
Long Short-Term Memory.
\textit{Neural Computation}, 9(8), 1735-1780.
% Long Short-Term Memory (LSTM)

\bibitem{bahdanau2014neural}
Bahdanau, D., Cho, K., & Bengio, Y. (2015).
Neural Machine Translation by Jointly Learning to Align and Translate.
\textit{ICLR 2015}.
% Attention Mechanism

\bibitem{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017).
Attention Is All You Need.
\textit{NeurIPS 2017}.
% Transformers

\end{thebibliography}

\end{frame}