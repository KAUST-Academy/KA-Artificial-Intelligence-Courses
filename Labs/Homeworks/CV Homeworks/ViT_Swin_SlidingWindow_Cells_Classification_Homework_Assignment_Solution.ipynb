{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T18:31:27.388408Z",
     "iopub.status.busy": "2025-07-31T18:31:27.388056Z",
     "iopub.status.idle": "2025-07-31T18:31:27.399191Z",
     "shell.execute_reply": "2025-07-31T18:31:27.397933Z",
     "shell.execute_reply.started": "2025-07-31T18:31:27.388381Z"
    }
   },
   "source": [
    "![Banner](https://i.imgur.com/a3uAqnb.png)\n",
    "\n",
    "# Cell Classification using ViT + Swin Transformers (Sliding-Window Approach)\n",
    "\n",
    "In this homework, we will classify biomedical cell images using two Vision Transformer architectures:\n",
    "- **ViT-B/16**\n",
    "- **Swin-T**\n",
    "\n",
    "Both backbones require inputs of size **224×224**, which is smaller than the actual image sizes. Instead of resizing (which may distort the cell structure), we adopt a **sliding-window** approach:\n",
    "- **Training**: we randomly crop 224×224 windows\n",
    "- **Validation**: we center crop 224×224\n",
    "- **Inference**: we slide a window over the full image and average the probabilities across windows\n",
    "\n",
    "Sliding window apporach is very useful if we have huge images sizes, or if we have different resolutions amongst the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"mohammad2012191/cells-types\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1️⃣ Load Data & Prepare Splits\n",
    "\n",
    "**Task**: Load the `data.csv` file, extract labels, and perform stratified train/val split.\n",
    "\n",
    "**ToDo**:\n",
    "- Read the CSV file and cast `cell_type` to string\n",
    "- Extract class names and build `label2idx` dictionary\n",
    "- Perform stratified split with `train_test_split`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:17:43.971297Z",
     "iopub.status.busy": "2025-07-31T19:17:43.971019Z",
     "iopub.status.idle": "2025-07-31T19:17:43.982186Z",
     "shell.execute_reply": "2025-07-31T19:17:43.981512Z",
     "shell.execute_reply.started": "2025-07-31T19:17:43.971277Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Load and split data\n",
    "csv_path = path + \"/data.csv\"  # expects id,cell_type\n",
    "df = pd.read_csv(csv_path)\n",
    "df['cell_type'] = df['cell_type'].astype(str)\n",
    "# Create label->index mapping\n",
    "types = sorted(df['cell_type'].unique())\n",
    "label2idx = {c: i for i, c in enumerate(types)}\n",
    "\n",
    "# Stratified train/val split\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df['cell_type'], random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Data Preprocessing\n",
    "\n",
    "**Task**: Define image transformations and implement a custom dataset class.\n",
    "\n",
    "**ToDo**:\n",
    "- Don't use Resize\n",
    "- Apply `RandomCrop(224)` during training\n",
    "- Apply `CenterCrop(224)` during validation (best we can do, we will apply sliding window for full image in inference)\n",
    "- Normalize using ImageNet stats\n",
    "- Load images from the `images/` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:17:44.390402Z",
     "iopub.status.busy": "2025-07-31T19:17:44.389742Z",
     "iopub.status.idle": "2025-07-31T19:17:44.398085Z",
     "shell.execute_reply": "2025-07-31T19:17:44.397404Z",
     "shell.execute_reply.started": "2025-07-31T19:17:44.390379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Transforms (no resizing; enforce 224x224 via crops)\n",
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "train_transform = T.Compose([\n",
    "    T.RandomCrop(224),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "val_transform = T.Compose([\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "# Dataset class\n",
    "doc_dir = path + \"/images\"  \n",
    "class CellDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform, label2idx):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.label2idx = label2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.df.loc[idx, 'id']\n",
    "        label = self.df.loc[idx, 'cell_type']\n",
    "        path = os.path.join(self.img_dir, f\"{img_id}.png\")\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, self.label2idx[label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Create DataLoaders\n",
    "\n",
    "**Task**: Load datasets using `DataLoader`.\n",
    "\n",
    "**ToDo**:\n",
    "- Use `shuffle=True` for training\n",
    "- Use `shuffle=False` for validation\n",
    "- Set batch size and workers\n",
    "- Define the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:25:46.732138Z",
     "iopub.status.busy": "2025-07-31T19:25:46.731287Z",
     "iopub.status.idle": "2025-07-31T19:25:46.738170Z",
     "shell.execute_reply": "2025-07-31T19:25:46.737413Z",
     "shell.execute_reply.started": "2025-07-31T19:25:46.732113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_ds = CellDataset(train_df, doc_dir, train_transform, label2idx)\n",
    "val_ds = CellDataset(val_df, doc_dir, val_transform, label2idx)\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Build ViT + Swin Combined Model\n",
    "\n",
    "**Task**: Create a model that extracts features from both backbones and concatenates them.\n",
    "\n",
    "**ToDo**:\n",
    "- Load ViT-B/16 (models.vit_b_16) and Swin-T (models.swin_t) with pretrained weights\n",
    "- Replace their heads with `nn.Identity` (i.e. remove the classifier heads)\n",
    "- Concatenate features and pass to a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:25:48.051765Z",
     "iopub.status.busy": "2025-07-31T19:25:48.051096Z",
     "iopub.status.idle": "2025-07-31T19:25:50.377248Z",
     "shell.execute_reply": "2025-07-31T19:25:50.376424Z",
     "shell.execute_reply.started": "2025-07-31T19:25:48.051740Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class VitSwinConcat(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        # ViT backbone\\        \n",
    "        self.vit = models.vit_b_16(pretrained=True)\n",
    "        # remove classification head\n",
    "        self.vit.heads = nn.Identity()\n",
    "        # Swin backbone\n",
    "        self.swin = models.swin_t(pretrained=True)\n",
    "        self.swin.head = nn.Identity()\n",
    "        # both backbones output 768-d features\n",
    "        self.classifier = nn.Linear(768 * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        f1 = self.vit(x)\n",
    "        f2 = self.swin(x)\n",
    "        f = torch.cat([f1, f2], dim=1)\n",
    "        return self.classifier(f)\n",
    "\n",
    "# instantiate\n",
    "n_classes = len(types)\n",
    "model = VitSwinConcat(n_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Train & Validate\n",
    "\n",
    "**Task**: Train the model and evaluate accuracy on the validation set.\n",
    "\n",
    "**ToDo**:\n",
    "- Write training and inference loops\n",
    "- Track training/validation loss and accuracy\n",
    "- Save the model at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:25:50.379709Z",
     "iopub.status.busy": "2025-07-31T19:25:50.379169Z",
     "iopub.status.idle": "2025-07-31T19:29:16.321097Z",
     "shell.execute_reply": "2025-07-31T19:29:16.319692Z",
     "shell.execute_reply.started": "2025-07-31T19:25:50.379675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6438b37a55b4c5fb688c0fc914b80cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 [Train]:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "602103cd12d04118b4e6e5dca423fbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 0 [Val]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b638f9bf600>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "Exception ignored in:   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x7b638f9bf600>    \n",
      "if w.is_alive():Traceback (most recent call last):\n",
      "\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "     self._shutdown_workers() \n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "       if w.is_alive(): \n",
      " ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "^^^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^^\n",
      "\n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "      assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
      "                 ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "^^AssertionError: can only test a child process\n",
      "^^^\n",
      "AssertionError: can only test a child process\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7b638f9bf600>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "      Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7b638f9bf600>\n",
      "^Traceback (most recent call last):\n",
      "^^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
      "^    ^^self._shutdown_workers()^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
      "^^    if w.is_alive():^\n",
      "^\n",
      "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "     assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
      "        ^^^^ ^ ^^ ^ ^^ ^ ^ \n",
      "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
      "    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
      "^  ^  ^^ ^  ^ ^ ^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError^: ^^can only test a child process\n",
      "^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train Loss 0.2313 | Val Loss 0.0310 | Val Acc 0.9903\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebf1a97f47f4b2f8b822c9cbbbf6477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Train]:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eda76a7c25345448b3574457c9440bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 [Val]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss 0.0304 | Val Loss 0.0123 | Val Acc 0.9981\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4050596a24d429b823eb08fb5dae0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Train]:   0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1d98a7024d4bf5a85209eeda1cd1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 [Val]:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss 0.0072 | Val Loss 0.0079 | Val Acc 0.9961\n"
     ]
    }
   ],
   "source": [
    "# 6. Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    # training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * imgs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    accuracy = correct / len(val_loader.dataset)\n",
    "    print(f\"Epoch {epoch}: Train Loss {epoch_loss:.4f} | Val Loss {val_loss:.4f} | Val Acc {accuracy:.4f}\")\n",
    "\n",
    "# save weights\n",
    "torch.save(model.state_dict(), \"vit_swin_concat.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Sliding-Window Inference\n",
    "\n",
    "**Task**: Write a function to classify a full image using sliding windows.\n",
    "\n",
    "**ToDo**:\n",
    "- Slide a 224×224 window with stride (e.g. 112)\n",
    "- Average softmax probabilities\n",
    "- Print individual patch predictions and final class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:22:40.871336Z",
     "iopub.status.busy": "2025-07-31T19:22:40.871067Z",
     "iopub.status.idle": "2025-07-31T19:22:41.179559Z",
     "shell.execute_reply": "2025-07-31T19:22:41.178910Z",
     "shell.execute_reply.started": "2025-07-31T19:22:40.871317Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch 0:  {'astro': 0.0003629255515988916, 'cort': 0.9995890259742737, 'shsy5y': 4.803628326044418e-05}\n",
      "Patch 1:  {'astro': 0.002050854032859206, 'cort': 0.9978287816047668, 'shsy5y': 0.00012039497960358858}\n",
      "Patch 2:  {'astro': 0.0003413844096940011, 'cort': 0.9996126294136047, 'shsy5y': 4.605785943567753e-05}\n",
      "Patch 3:  {'astro': 0.0004650430055335164, 'cort': 0.9994938373565674, 'shsy5y': 4.10635257139802e-05}\n",
      "Patch 4:  {'astro': 0.001034583430737257, 'cort': 0.9988666772842407, 'shsy5y': 9.87577805062756e-05}\n",
      "Patch 5:  {'astro': 0.0003107638331130147, 'cort': 0.999643087387085, 'shsy5y': 4.611604526871815e-05}\n",
      "Patch 6:  {'astro': 0.00019685731967911124, 'cort': 0.9997627139091492, 'shsy5y': 4.044066372443922e-05}\n",
      "Patch 7:  {'astro': 0.00017888678121380508, 'cort': 0.9997997879981995, 'shsy5y': 2.136872717528604e-05}\n",
      "Patch 8:  {'astro': 0.00012940005399286747, 'cort': 0.999840259552002, 'shsy5y': 3.0311841328511946e-05}\n",
      "Patch 9:  {'astro': 0.0006890001241117716, 'cort': 0.9992245435714722, 'shsy5y': 8.649512892588973e-05}\n",
      "Patch 10:  {'astro': 0.0007399533060379326, 'cort': 0.9991267323493958, 'shsy5y': 0.0001333835971308872}\n",
      "Patch 11:  {'astro': 0.0006706351414322853, 'cort': 0.9992683529853821, 'shsy5y': 6.103111445554532e-05}\n",
      "Patch 12:  {'astro': 0.00020842120284214616, 'cort': 0.9997585415840149, 'shsy5y': 3.304508572909981e-05}\n",
      "Patch 13:  {'astro': 0.0001657304965192452, 'cort': 0.9998027682304382, 'shsy5y': 3.147909228573553e-05}\n",
      "Patch 14:  {'astro': 0.0004247387987561524, 'cort': 0.9994537234306335, 'shsy5y': 0.00012151007103966549}\n",
      "Average:  {'astro': 0.0005312784924171865, 'cort': 0.9994047284126282, 'shsy5y': 6.39661229797639e-05}\n",
      "Final class:  cort\n"
     ]
    }
   ],
   "source": [
    "# Inference with sliding window\n",
    "def inference_sliding_window(model, img_path, window_size=224, stride=112, device=device):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert('RGB')\n",
    "    w, h = img.size\n",
    "    to_tensor = T.ToTensor()\n",
    "    normalize = T.Normalize(mean, std)\n",
    "    probs = []\n",
    "    # slide\n",
    "    for y in range(0, h - window_size + 1, stride):\n",
    "        for x in range(0, w - window_size + 1, stride):\n",
    "            patch = img.crop((x, y, x + window_size, y + window_size))\n",
    "            tensor = normalize(to_tensor(patch)).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                out = model(tensor)\n",
    "                p = torch.softmax(out, dim=1).cpu().numpy()[0]\n",
    "            probs.append(p)\n",
    "    probs = np.stack(probs, axis=0)\n",
    "    avg_prob = probs.mean(axis=0)\n",
    "    final_idx = int(avg_prob.argmax())\n",
    "    # print per-patch and average\n",
    "    for i, p in enumerate(probs):\n",
    "        print(f\"Patch {i}: \", {types[j]: float(p[j]) for j in range(len(types))})\n",
    "    print(\"Average: \", {types[j]: float(avg_prob[j]) for j in range(len(types))})\n",
    "    print(\"Final class: \", types[final_idx])\n",
    "\n",
    "\n",
    "inference_sliding_window(model, path + \"images/5.png\")  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
