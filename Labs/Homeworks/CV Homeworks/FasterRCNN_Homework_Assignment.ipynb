{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Banner](https://i.imgur.com/a3uAqnb.png)\n",
    "\n",
    "# Vehicle Detection using Faster R-CNN - Homework Assignment\n",
    "\n",
    "![Faster R-CNN Architecture](https://wiki.cloudfactory.com/media/pages/docs/mp-wiki/model-architectures/faster-r-cnn/d1436cdae7-1684131962/image-15.webp)\n",
    "\n",
    "In this homework, you will implement a **Faster R-CNN** model for detecting different types of vehicles in images. This is a two-stage object detection architecture that combines region proposal networks with classification.\n",
    "\n",
    "## üìå Project Overview\n",
    "- **Task**: Multi-class vehicle detection (Car, Bus, Truck, Motorcycle, Ambulance)\n",
    "- **Architecture**: Faster R-CNN with MobileNet backbone\n",
    "- **Dataset**: Vehicles OpenImages dataset (from Roboflow)\n",
    "- **Goal**: Detect and classify vehicles with bounding box predictions\n",
    "\n",
    "## üìö Learning Objectives\n",
    "By completing this assignment, you will:\n",
    "- Understand two-stage object detection architectures\n",
    "- Learn about Region Proposal Networks (RPN) and ROI pooling\n",
    "- Implement transfer learning for object detection\n",
    "- Practice working with COCO format annotations\n",
    "- Evaluate object detection models using mAP metrics\n",
    "- Visualize detection results with confidence thresholds\n",
    "\n",
    "## üéØ Evaluation Metrics\n",
    "You will be evaluated using:\n",
    "- **mAP@0.5:0.95**: Mean Average Precision across IoU thresholds 0.5-0.95\n",
    "- **mAP@0.5**: Mean Average Precision at IoU threshold 0.5\n",
    "- **mAP@0.75**: Mean Average Precision at IoU threshold 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Dataset Setup and Library Imports\n",
    "\n",
    "**Task**: Import necessary libraries and download the vehicle detection dataset.\n",
    "\n",
    "**Requirements**:\n",
    "- Import PyTorch, torchvision, and related libraries\n",
    "- Set up device configuration (GPU/CPU)\n",
    "- Download the Vehicles OpenImages dataset using Roboflow API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:20:53.086173Z",
     "start_time": "2025-07-20T08:20:50.985661Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Import all necessary libraries\n",
    "# Required imports: torch, torchvision, DataLoader, Dataset, transforms\n",
    "# Additional imports: os, json, PIL, numpy, roboflow, matplotlib, tqdm\n",
    "\n",
    "# TODO: Check device availability and set device\n",
    "# Use \"cuda\" if available, otherwise \"cpu\"\n",
    "\n",
    "# TODO: Set random seeds for reproducibility (use seed=42)\n",
    "\n",
    "# TODO: Print device information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Download Vehicle Dataset\n",
    "\n",
    "**Task**: Use Roboflow API to download the vehicle detection dataset.\n",
    "\n",
    "**Requirements**:\n",
    "- Use your own API key to access Roboflow\n",
    "- Download the \"vehicles-openimages\" dataset in COCO format\n",
    "- Store the dataset path for later use\n",
    "\n",
    "**Note**: The dataset contains images with bounding box annotations for 6 vehicle classes.(One of them is not used but it exists.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:21:45.428984Z",
     "start_time": "2025-07-20T08:21:43.325068Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Import Roboflow and initialize with API key\n",
    "# Use your own API key:\n",
    "# TODO: Access workspace \"roboflow-gw7yv\" and project \"vehicles-openimages\"\n",
    "# TODO: Download version 1 in \"coco\" format\n",
    "# TODO: Store the dataset location path\n",
    "# TODO: Print the dataset path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Data Exploration and Class Setup\n",
    "\n",
    "**Task**: Explore the dataset structure and set up class mappings.\n",
    "\n",
    "**Requirements**:\n",
    "- Load COCO annotation files for train, validation, and test sets\n",
    "- Extract category information and create class mappings\n",
    "- Print the available vehicle classes\n",
    "- Understand the dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T08:21:48.104992Z",
     "start_time": "2025-07-20T08:21:48.101167Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Create paths to annotation files\n",
    "# Path structure: datasetPath/[split]/_annotations.coco.json\n",
    "# Create train_annotations, val_annotations, test_annotations paths\n",
    "\n",
    "# TODO: Load training annotations JSON file\n",
    "# Extract categories information from the COCO format\n",
    "\n",
    "# TODO: Create class mappings\n",
    "# Create: class_names list, id_to_class dict, class_to_id dict\n",
    "# Sort categories by ID for consistent ordering\n",
    "\n",
    "# TODO: Print the number of classes and class names\n",
    "# TODO: Print a sample of the class mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Custom Dataset Class\n",
    "\n",
    "**Task**: Create a PyTorch Dataset class to handle the vehicle detection data.\n",
    "\n",
    "**Requirements**:\n",
    "- Inherit from torch.utils.data.Dataset\n",
    "- Parse COCO format annotations\n",
    "- Return images and targets in the format expected by Faster R-CNN\n",
    "- Handle bounding box coordinate conversion (COCO to PyTorch format)\n",
    "- Include proper target dictionary with required keys\n",
    "\n",
    "**Expected Target Format**:\n",
    "```python\n",
    "target = {\n",
    "    'boxes': tensor([[x1, y1, x2, y2], ...]),  # Bounding boxes\n",
    "    'labels': tensor([class_id1, class_id2, ...]),  # Class labels\n",
    "    'image_id': tensor([image_id]),  # Image identifier\n",
    "    'area': tensor([area1, area2, ...]),  # Box areas\n",
    "    'iscrowd': tensor([0, 0, ...])  # Crowd annotations (all 0)\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create VehicleDataset class inheriting from Dataset\n",
    "\n",
    "# TODO: Implement __init__(self, root_dir, annotation_file, transform=None):\n",
    "#       - Store root_dir, annotation_file, and transform\n",
    "#       - Load COCO annotations from JSON file\n",
    "#       - Create image ID to image info mapping\n",
    "#       - Create category ID to name mapping\n",
    "#       - Group annotations by image_id\n",
    "#       - Store list of image_ids that have annotations\n",
    "\n",
    "# TODO: Implement __len__(self):\n",
    "#       - Return the number of images with annotations\n",
    "\n",
    "# TODO: Implement __getitem__(self, idx):\n",
    "#       - Get image_id from index\n",
    "#       - Load image using PIL and convert to RGB\n",
    "#       - Get all annotations for this image\n",
    "#       - Convert COCO bbox format [x, y, width, height] to [x1, y1, x2, y2]\n",
    "#       - Create boxes tensor (float32) and labels tensor (int64)\n",
    "#       - Calculate areas for each box\n",
    "#       - Create target dictionary with required keys\n",
    "#       - Apply transform to image if provided\n",
    "#       - Return (image, target) tuple\n",
    "\n",
    "# TODO: Create transform pipeline:\n",
    "#       - Use transforms.Compose with transforms.ToTensor()\n",
    "\n",
    "# TODO: Create dataset instances for train, validation, and test\n",
    "# TODO: Create a custom collate function for DataLoader\n",
    "# TODO: Create DataLoaders with appropriate batch sizes and settings\n",
    "# TODO: Print dataset sizes and test with one sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Data Visualization\n",
    "\n",
    "**Task**: Visualize sample images with ground truth annotations.\n",
    "\n",
    "**Requirements**:\n",
    "- Create a visualization function that displays images with bounding boxes\n",
    "- Use different colors for different vehicle classes\n",
    "- Show class labels and bounding boxes clearly\n",
    "- Display multiple samples from training and validation sets\n",
    "\n",
    "**Color Scheme**:\n",
    "- Vehicle (class 0): Red\n",
    "- Ambulance (class 1): Blue  \n",
    "- Bus (class 2): Green\n",
    "- Car (class 3): Orange\n",
    "- Motorcycle (class 4): Purple\n",
    "- Truck (class 5): Brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import matplotlib.pyplot and matplotlib.patches\n",
    "# TODO: Import torchvision.transforms.functional\n",
    "\n",
    "# TODO: Define vehicle class names dictionary (0: 'vehicles', 1: 'Ambulance', etc.)\n",
    "# TODO: Define class colors dictionary for visualization\n",
    "\n",
    "# TODO: Create visualize_sample function that:\n",
    "#       - Takes dataset, list of indices, and title as parameters\n",
    "#       - Creates a 2x3 subplot grid\n",
    "#       - For each index:\n",
    "#         * Get image and target from dataset\n",
    "#         * Convert tensor image to PIL format\n",
    "#         * Draw bounding boxes using matplotlib patches\n",
    "#         * Add class labels with colored text\n",
    "#         * Set appropriate title for each subplot\n",
    "#       - Display the complete grid with main title\n",
    "\n",
    "# TODO: Visualize training samples with indices [0, 10, 25, 50, 75, 100]\n",
    "# TODO: Visualize validation samples with indices [0, 5, 10, 15, 20, 25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Model Architecture Setup\n",
    "\n",
    "**Task**: Set up the Faster R-CNN model with transfer learning.\n",
    "\n",
    "**Requirements**:\n",
    "- Use a pre-trained Faster R-CNN model with MobileNet backbone\n",
    "- Modify the classifier head for the number of vehicle classes\n",
    "- Implement selective fine-tuning (freeze backbone, train detection heads)\n",
    "- Move model to appropriate device (GPU/CPU)\n",
    "\n",
    "**Architecture Details**:\n",
    "- **Backbone**: MobileNet V3 Large with FPN (Feature Pyramid Network)\n",
    "- **RPN**: Region Proposal Network for object proposals\n",
    "- **ROI Head**: Classification and regression head for final predictions\n",
    "- **Classes**: 6 vehicle classes (including background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import required torchvision modules\n",
    "# Import: torchvision.models.detection.faster_rcnn.FastRCNNPredictor\n",
    "\n",
    "# TODO: Load pre-trained Faster R-CNN model\n",
    "# Use: torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "\n",
    "# TODO: Modify the classifier head\n",
    "# Get input features from model.roi_heads.box_predictor.cls_score.in_features\n",
    "# Replace box_predictor with FastRCNNPredictor(in_features, num_classes)\n",
    "# Set num_classes = 6 (5 vehicle classes + background)\n",
    "\n",
    "# TODO: Implement selective fine-tuning\n",
    "# Freeze all model parameters: model.requires_grad_(False)\n",
    "# Unfreeze detection heads: model.roi_heads.box_predictor.requires_grad_(True)\n",
    "# Unfreeze RPN: model.rpn.requires_grad_(True)\n",
    "\n",
    "# TODO: Move model to device\n",
    "# TODO: Print model summary and number of trainable parameters\n",
    "\n",
    "# TODO: Define vehicle classes dictionary for reference\n",
    "# TODO: Create reverse mapping from class names to IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Training Functions and Metrics\n",
    "\n",
    "**Task**: Implement training and validation functions with proper metrics.\n",
    "\n",
    "**Requirements**:\n",
    "- Create training function that handles loss computation and backpropagation\n",
    "- Implement validation function using Mean Average Precision (mAP)\n",
    "- Use torchmetrics for proper object detection evaluation\n",
    "- Display training progress with progress bars\n",
    "- Return meaningful metrics for monitoring\n",
    "\n",
    "**Key Concepts**:\n",
    "- **mAP@0.5:0.95**: Average mAP across IoU thresholds from 0.5 to 0.95\n",
    "- **mAP@0.5**: mAP at IoU threshold 0.5 (PASCAL VOC style)\n",
    "- **mAP@0.75**: mAP at IoU threshold 0.75 (stricter evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import tqdm for progress bars and torchmetrics for evaluation\n",
    "# Import: from tqdm import tqdm\n",
    "# Import: from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "# TODO: Implement train_one_epoch function:\n",
    "#       - Set model to training mode\n",
    "#       - Initialize total_loss and progress bar\n",
    "#       - For each batch:\n",
    "#         * Move images and targets to device\n",
    "#         * Forward pass (model returns loss_dict in training mode)\n",
    "#         * Sum all losses from loss_dict\n",
    "#         * Backpropagate and update optimizer\n",
    "#         * Track running average loss\n",
    "#         * Update progress bar with current and average loss\n",
    "#       - Return average loss for the epoch\n",
    "\n",
    "# TODO: Implement validate_model function:\n",
    "#       - Set model to evaluation mode\n",
    "#       - Initialize MeanAveragePrecision metric with IoU thresholds\n",
    "#       - Use torch.no_grad() context\n",
    "#       - For each batch:\n",
    "#         * Move images to device\n",
    "#         * Get model predictions\n",
    "#         * Convert predictions and targets to CPU\n",
    "#         * Update metric with predictions and targets\n",
    "#       - Compute and return final metrics\n",
    "\n",
    "# TODO: Set up optimizer and scheduler\n",
    "# Use: torch.optim.AdamW with learning rate 0.0001 and weight_decay 0.0005\n",
    "# Use: torch.optim.lr_scheduler.StepLR with step_size=3 and gamma=0.1\n",
    "# Only optimize parameters that require gradients\n",
    "\n",
    "# TODO: Test the functions with a small batch to ensure they work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Model Training\n",
    "\n",
    "**Task**: Train the Faster R-CNN model on the vehicle detection dataset.\n",
    "\n",
    "**Requirements**:\n",
    "- Train for 5 epochs with progress monitoring\n",
    "- Track training loss and validation mAP metrics\n",
    "- Save the best model based on validation mAP\n",
    "- Display training progress and timing information\n",
    "- Plot training curves for analysis\n",
    "\n",
    "**Training Strategy**:\n",
    "- **Epochs**: 5 (adjust based on computational resources)\n",
    "- **Learning Rate**: 0.0001 with step decay\n",
    "- **Batch Size**: 2 (adjust based on GPU memory)\n",
    "- **Optimization**: AdamW with weight decay\n",
    "- **Best Model**: Save based on highest validation mAP@0.5:0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set training hyperparameters\n",
    "# num_epochs = 5, best_map = 0.0\n",
    "\n",
    "# TODO: Initialize training history dictionary\n",
    "# Track: train_loss, val_map, val_map_50, val_map_75\n",
    "\n",
    "# TODO: Implement main training loop:\n",
    "#       - Start timing for total training time\n",
    "#       - For each epoch:\n",
    "#         * Record epoch start time\n",
    "#         * Call train_one_epoch function\n",
    "#         * Call validate_model function\n",
    "#         * Step the learning rate scheduler\n",
    "#         * Get current learning rate\n",
    "#         * Extract metric values (map, map_50, map_75)\n",
    "#         * Update training history\n",
    "#         * Calculate epoch time\n",
    "#         * Print comprehensive epoch results\n",
    "#         * Save best model if validation mAP improves\n",
    "#       - Print total training time and best performance\n",
    "\n",
    "# TODO: Create visualization of training progress\n",
    "# Plot 3 subplots:\n",
    "# 1. Training loss over epochs\n",
    "# 2. Validation mAP metrics (mAP@0.5:0.95, mAP@0.5, mAP@0.75)  \n",
    "# 3. Normalized comparison of loss and mAP\n",
    "\n",
    "# TODO: Save training history and model checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Model Evaluation and Testing\n",
    "\n",
    "**Task**: Evaluate the trained model on the test set and visualize predictions.\n",
    "\n",
    "**Requirements**:\n",
    "- Load the best saved model\n",
    "- Evaluate on the test set using the same metrics\n",
    "- Visualize predictions vs ground truth on test images\n",
    "- Show the effect of different confidence thresholds\n",
    "- Analyze model performance across different vehicle classes\n",
    "\n",
    "**Visualization Requirements**:\n",
    "- Display ground truth boxes in red\n",
    "- Display predicted boxes in green\n",
    "- Show confidence scores for predictions\n",
    "- Compare predictions at different confidence thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the best saved model\n",
    "# Load checkpoint and restore model state_dict\n",
    "\n",
    "# TODO: Evaluate on test set\n",
    "# Use validate_model function with test_loader\n",
    "# Print test set results (mAP@0.5:0.95, mAP@0.5, mAP@0.75)\n",
    "\n",
    "# TODO: Implement visualize_predictions function:\n",
    "#       - Set model to evaluation mode\n",
    "#       - Create 2x3 subplot grid\n",
    "#       - For each test image:\n",
    "#         * Generate predictions using model\n",
    "#         * Filter predictions by confidence threshold\n",
    "#         * Display original image\n",
    "#         * Draw ground truth boxes (red) with \"GT:\" labels\n",
    "#         * Draw predicted boxes (green) with \"Pred:\" labels and confidence scores\n",
    "#         * Set appropriate titles showing object counts\n",
    "\n",
    "# TODO: Visualize predictions on test indices [0, 5, 10, 15, 20, 25]\n",
    "# Use confidence threshold 0.5\n",
    "\n",
    "# TODO: Create confidence threshold comparison\n",
    "# Show same image with thresholds [0.3, 0.5, 0.7]\n",
    "# Display in 1x3 subplot showing effect of threshold on detections\n",
    "\n",
    "# TODO: Print analysis of results and model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Evaluation Criteria\n",
    "\n",
    "Your homework will be evaluated based on:\n",
    "\n",
    "### 1. Implementation Correctness (40%)\n",
    "- **Dataset Loading**: Proper COCO format parsing and PyTorch dataset implementation\n",
    "- **Model Architecture**: Correct Faster R-CNN setup with transfer learning\n",
    "- **Training Loop**: Working training and validation with appropriate loss handling\n",
    "- **Evaluation**: Proper mAP computation and metric tracking\n",
    "\n",
    "### 2. Training and Results (30%)\n",
    "- **Model Training**: Successful training without errors\n",
    "- **Convergence**: Reasonable loss curves and metric improvement\n",
    "- **Performance**: Achieving meaningful detection results on test set\n",
    "- **Hyperparameters**: Appropriate choice of learning rate, batch size, etc.\n",
    "\n",
    "### 3. Code Quality and Documentation (30%)\n",
    "- **Code Structure**: Clean, readable code with proper organization\n",
    "- **Comments**: Adequate documentation explaining key steps\n",
    "- **Error Handling**: Robust implementation handling edge cases\n",
    "- **Efficiency**: Reasonable computational complexity"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
