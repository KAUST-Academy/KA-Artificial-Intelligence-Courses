{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b53b4dac",
   "metadata": {},
   "source": [
    "![Banner](https://i.imgur.com/a3uAqnb.png)\n",
    "\n",
    "# Image Translation using Conditional GAN (Pix2Pix) - Homework Assignment\n",
    "\n",
    "In this homework, you will implement a **Conditional Image-to-Image GAN** for translating edge images to shoe images. This is based on the **Pix2Pix** architecture that learns to map from one image domain to another.\n",
    "\n",
    "## üìå Project Overview\n",
    "- **Task**: Edge-to-Shoe image translation\n",
    "- **Architecture**: Conditional GAN with U-Net Generator and PatchGAN Discriminator\n",
    "- **Dataset**: Edge2Shoes dataset (provided)\n",
    "- **Goal**: Generate realistic shoe images from edge sketches\n",
    "\n",
    "## üìö Learning Objectives\n",
    "By completing this assignment, you will:\n",
    "- Understand conditional GANs and image-to-image translation\n",
    "- Implement U-Net architecture with skip connections\n",
    "- Build a PatchGAN discriminator\n",
    "- Learn about combined loss functions (adversarial + L1)\n",
    "- Practice training GANs with proper loss balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f022d71",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Dataset Setup (PROVIDED)\n",
    "\n",
    "The Edge2Shoes dataset has been downloaded and prepared for you. The dataset structure is as follows:\n",
    "- `train/` folder contains training images\n",
    "- `val/` folder contains validation images\n",
    "- Each image contains edge sketch (left half) and corresponding shoe (right half)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeecc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Dataset already downloaded and prepared\n",
    "load_dotenv()\n",
    "path = kagglehub.dataset_download(\"balraj98/edges2shoes-dataset\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dd0a00",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Import Libraries and Configuration\n",
    "\n",
    "**Task**: Import all necessary libraries and set up configuration parameters.\n",
    "\n",
    "**Requirements**:\n",
    "- Import PyTorch, torchvision, and related libraries\n",
    "- Import matplotlib, PIL, numpy, and other utilities\n",
    "- Set random seeds for reproducibility\n",
    "- Configure hyperparameters with reasonable values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b33c2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import all necessary libraries\n",
    "\n",
    "# TODO: Set random seeds for reproducibility (use seed=42)\n",
    "\n",
    "# TODO: Check device availability and print\n",
    "\n",
    "# TODO: Define configuration parameters:\n",
    "IMG_SIZE = 128  # Image size \n",
    "BATCH_SIZE = 16  # Batch size\n",
    "LEARNING_RATE = 0.0002  # Learning rate\n",
    "BETA1 = 0.5  # Adam optimizer beta1\n",
    "BETA2 = 0.999  # Adam optimizer beta2  \n",
    "LAMBDA_L1 = 100  # Weight for L1 loss\n",
    "NUM_EPOCHS = 5  # Number of training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2ff29e",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Custom Dataset Class\n",
    "\n",
    "**Task**: Create a custom dataset class that handles the Edge2Shoes data format.\n",
    "\n",
    "**Requirements**:\n",
    "- Split each image into left half (edge) and right half (shoe)\n",
    "- Apply transformations to both images\n",
    "- Return edge image as input and shoe image as target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc5a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create EdgeShoeDataset class inheriting from torch.utils.data.Dataset\n",
    "# TODO: In __init__:\n",
    "#       - Store root_dir and transform\n",
    "#       - Get list of all .jpg files\n",
    "# TODO: Implement __len__ to return number of images\n",
    "# TODO: Implement __getitem__ to:\n",
    "#       - Load image and convert to RGB\n",
    "#       - Split into left half (edge) and right half (shoe)\n",
    "#       - Apply transforms if provided\n",
    "#       - Return (edge_img, shoe_img) tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483c282",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Preprocessing and Loading\n",
    "\n",
    "**Task**: Set up data transformations and create data loaders.\n",
    "\n",
    "**Requirements**:\n",
    "- Resize images to target size (128x128)\n",
    "- Convert to tensors and normalize to [-1, 1] range\n",
    "- Create train and validation datasets and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08cf9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define transforms using transforms.Compose:\n",
    "#       - Resize to (IMG_SIZE, IMG_SIZE)\n",
    "#       - ToTensor() \n",
    "#       - Normalize with mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]\n",
    "\n",
    "# TODO: Create train_dataset using EdgeShoeDataset with train folder\n",
    "# TODO: Create val_dataset using EdgeShoeDataset with val folder\n",
    "\n",
    "# TODO: Create train_loader and val_loader with DataLoader\n",
    "#       - Use appropriate batch_size and shuffle settings\n",
    "\n",
    "# TODO: Print dataset sizes\n",
    "# TODO: Test by loading one sample and printing shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59c221",
   "metadata": {},
   "source": [
    "\n",
    "## 5Ô∏è‚É£ Generator Network (U-Net Architecture)\n",
    "\n",
    "**Task**: Implement a U-Net generator with encoder-decoder structure and skip connections.\n",
    "\n",
    "**Requirements**:\n",
    "- Encoder: Progressive downsampling using Conv2d layers\n",
    "- Decoder: Progressive upsampling using ConvTranspose2d layers  \n",
    "- Skip connections between corresponding encoder-decoder layers\n",
    "- Final output uses Tanh activation for [-1,1] range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae291bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create Generator class inheriting from nn.Module\n",
    "# TODO: In __init__(self, in_channels=3, out_channels=3):\n",
    "#       \n",
    "#       Build Encoder (downsampling):\n",
    "#       - Layer 1: Conv2d(3, 64, 4, 2, 1) + LeakyReLU (no BatchNorm)\n",
    "#       - Layer 2: Conv2d(64, 128, 4, 2, 1) + BatchNorm2d + LeakyReLU  \n",
    "#       - Layer 3: Conv2d(128, 256, 4, 2, 1) + BatchNorm2d + LeakyReLU\n",
    "#       - Layer 4: Conv2d(256, 512, 4, 2, 1) + BatchNorm2d + LeakyReLU\n",
    "#       - Layer 5: Conv2d(512, 512, 4, 2, 1) + BatchNorm2d + LeakyReLU\n",
    "#       - Layer 6: Conv2d(512, 512, 4, 2, 1) + BatchNorm2d + LeakyReLU (bottleneck)\n",
    "#\n",
    "#       Build Decoder (upsampling with skip connections):\n",
    "#       - Layer 1: ConvTranspose2d(512, 512, 4, 2, 1) + BatchNorm2d + Dropout + ReLU\n",
    "#       - Layer 2: ConvTranspose2d(1024, 512, 4, 2, 1) + BatchNorm2d + Dropout + ReLU\n",
    "#       - Layer 3: ConvTranspose2d(1024, 256, 4, 2, 1) + BatchNorm2d + ReLU\n",
    "#       - Layer 4: ConvTranspose2d(512, 128, 4, 2, 1) + BatchNorm2d + ReLU\n",
    "#       - Layer 5: ConvTranspose2d(256, 64, 4, 2, 1) + BatchNorm2d + ReLU\n",
    "#       - Final: ConvTranspose2d(128, 3, 4, 2, 1) + Tanh\n",
    "#\n",
    "# TODO: In forward(self, x):\n",
    "#       - Pass through encoder, save intermediate outputs\n",
    "#       - Pass through decoder, concatenating skip connections\n",
    "#       - Return final output\n",
    "#\n",
    "# TODO: Initialize generator and print parameter count\n",
    "# TODO: Test with random input to verify output shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2589ce",
   "metadata": {},
   "source": [
    "\n",
    "## 6Ô∏è‚É£ Discriminator Network (PatchGAN)\n",
    "\n",
    "**Task**: Implement a PatchGAN discriminator that classifies image patches as real/fake.\n",
    "\n",
    "**Requirements**:\n",
    "- Accept concatenated input (edge + shoe = 6 channels)\n",
    "- Use strided convolutions for downsampling\n",
    "- Output a patch-wise classification matrix (not single value)\n",
    "- Use LeakyReLU activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060b2d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create Discriminator class inheriting from nn.Module\n",
    "# TODO: In __init__(self, in_channels=6):  # 3 for edge + 3 for shoe\n",
    "#       Build discriminator layers:\n",
    "#       - Layer 1: Conv2d(6, 64, 4, 2, 1) + LeakyReLU (no BatchNorm)\n",
    "#       - Layer 2: Conv2d(64, 128, 4, 2, 1) + BatchNorm2d + LeakyReLU\n",
    "#       - Layer 3: Conv2d(128, 256, 4, 2, 1) + BatchNorm2d + LeakyReLU  \n",
    "#       - Layer 4: Conv2d(256, 512, 4, 1, 1) + BatchNorm2d + LeakyReLU\n",
    "#       - Final: Conv2d(512, 1, 4, 1, 1) (no activation)\n",
    "#\n",
    "# TODO: In forward(self, img_A, img_B):\n",
    "#       - Concatenate img_A and img_B along channel dimension\n",
    "#       - Pass through discriminator layers\n",
    "#       - Return patch predictions\n",
    "#\n",
    "# TODO: Initialize discriminator and print parameter count  \n",
    "# TODO: Test with random inputs to verify output shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d398bffa",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Loss Functions and Optimizers\n",
    "\n",
    "**Task**: Set up loss functions and optimizers for GAN training.\n",
    "\n",
    "**Requirements**:\n",
    "- Use appropriate loss functions for adversarial and reconstruction objectives\n",
    "- Initialize optimizers with given hyperparameters\n",
    "- Implement weight initialization for stable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f393036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define loss functions:\n",
    "#       - criterion_GAN = nn.BCEWithLogitsLoss() for adversarial loss\n",
    "#       - criterion_L1 = nn.L1Loss() for reconstruction loss\n",
    "\n",
    "# TODO: Create optimizers:\n",
    "#       - optimizer_G for generator with Adam(lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "#       - optimizer_D for discriminator with Adam(lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "\n",
    "# TODO: Implement weights_init(m) function:\n",
    "#       - For Conv layers: init with normal_(mean=0.0, std=0.02)\n",
    "#       - For BatchNorm layers: weight normal_(1.0, 0.02), bias constant_(0)\n",
    "\n",
    "# TODO: Apply weights_init to both generator and discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4bea33",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Training Loop\n",
    "\n",
    "**Task**: Implement the main GAN training loop with alternating updates.\n",
    "\n",
    "**Requirements**:\n",
    "- Train generator to fool discriminator and match target images\n",
    "- Train discriminator to distinguish real from generated images\n",
    "- Balance adversarial loss with L1 reconstruction loss\n",
    "- Track and display training progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91741e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create training loop for NUM_EPOCHS:\n",
    "#       \n",
    "#       For each batch in train_loader:\n",
    "#       - Move edge_imgs and real_shoes to device\n",
    "#       - Get batch_size\n",
    "#       \n",
    "#       Train Generator:\n",
    "#       - Generate fake_shoes from edge_imgs\n",
    "#       - Get discriminator prediction on (edge_imgs, fake_shoes)\n",
    "#       - Calculate adversarial loss (try to fool discriminator)\n",
    "#       - Calculate L1 loss between fake_shoes and real_shoes  \n",
    "#       - Total loss = adversarial_loss + LAMBDA_L1 * L1_loss\n",
    "#       - Backpropagate and update generator\n",
    "#       \n",
    "#       Train Discriminator:\n",
    "#       - Get prediction on real pair (edge_imgs, real_shoes)\n",
    "#       - Get prediction on fake pair (edge_imgs, fake_shoes.detach())\n",
    "#       - Calculate loss for real (should predict 1) and fake (should predict 0)\n",
    "#       - Total loss = (real_loss + fake_loss) / 2\n",
    "#       - Backpropagate and update discriminator\n",
    "#       \n",
    "# TODO: Use appropriate label creation for discriminator training\n",
    "# TODO: Track losses and display training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffe12b7",
   "metadata": {},
   "source": [
    "\n",
    "## 9Ô∏è‚É£ Evaluation and Visualization\n",
    "\n",
    "**Task**: Evaluate your trained model and visualize results.\n",
    "\n",
    "**Requirements**:\n",
    "- Generate shoes from validation edge images\n",
    "- Compare with ground truth shoes\n",
    "- Create side-by-side visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac4c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set models to evaluation mode\n",
    "# TODO: Create function to denormalize images from [-1,1] to [0,1]\n",
    "# TODO: Create visualization function that:\n",
    "#       - Takes several validation samples\n",
    "#       - Generates fake shoes using trained generator\n",
    "#       - Displays edge input, generated output, and real target\n",
    "#       - Shows results in a grid format (3 columns: edge, generated, real)\n",
    "# TODO: Display results for 10 validation samples\n",
    "# TODO: Plot training loss curves for generator and discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c68346",
   "metadata": {},
   "source": [
    "## üîü Analysis\n",
    "\n",
    "**Task**: Analyze your results\n",
    "\n",
    "**Requirements**:\n",
    "- Evaluate the quality of generated images\n",
    "- Discuss strengths and limitations of your model\n",
    "- Test the effect of different hyperparameters (optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6976a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze the quality of generated shoes, for example: \n",
    "#       - Are edges properly converted to realistic shoes?\n",
    "#       - Do shoes maintain the shape from edge inputs?\n",
    "#       - How realistic do the textures and colors look?\n",
    "# TODO: Document any interesting observations or failure cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c8434",
   "metadata": {},
   "source": [
    "\n",
    "## üìù Evaluation Criteria\n",
    "\n",
    "Your homework will be evaluated based on:\n",
    "\n",
    "1. **Implementation Correctness (40%)**\n",
    "   - Proper U-Net generator implementation\n",
    "   - Correct PatchGAN discriminator\n",
    "   - Working training loop with appropriate losses\n",
    "\n",
    "2. **Training and Results (30%)**\n",
    "   - Model trains without errors\n",
    "   - Reasonable loss convergence\n",
    "   - Generated images show edge-to-shoe translation\n",
    "\n",
    "3. **Code Quality (20%)**\n",
    "   - Clean, readable code with comments\n",
    "   - Proper tensor shapes and data flow\n",
    "   - Efficient implementation\n",
    "\n",
    "4. **Analysis (10%)**\n",
    "   - Discussion of results\n",
    "   - Understanding of model behavior\n",
    "   - Insights about GAN training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
