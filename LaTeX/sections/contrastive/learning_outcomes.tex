\begin{frame}[allowframebreaks]{Contrastive Learning: Learning Outcomes}
    By the end of this lecture, you will be able to:
    \begin{itemize}
        \setlength{\itemsep}{-0.1em}
        \item Trace the exciting journey of contrastive learning, understanding how innovations evolved from MoCo $\rightarrow$ SimCLR $\rightarrow$ BYOL $\rightarrow$ DINO $\rightarrow$ iBOT.
        \item Spot the real-world trade-offs in model design—like balancing memory and compute, or choosing between negative sampling and avoiding Bayesian collapse.
        \item Derive and visualize how batch size and temperature ($\tau$) shape the spread of learned embeddings.
        \item Explain in simple terms why we drop the projection head when moving to downstream tasks—and why that’s a smart move!
        \item Analyze the clever tricks BYOL uses to avoid representational collapse, keeping its features meaningful.
        \item Illustrate why centering and sharpening the teacher’s outputs are essential for effective learning.
    \end{itemize}
\end{frame}