{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCUKQYfDA9a2"
      },
      "source": [
        "![Banner](https://i.imgur.com/a3uAqnb.png)\n",
        "\n",
        "# CycleGAN for Selfie-to-Anime Translation - Homework Assignment\n",
        "\n",
        "In this homework, you will implement a **CycleGAN** for unpaired image-to-image translation between selfie and anime domains. CycleGANs enable learning mappings between two domains without requiring paired training examples.\n",
        "\n",
        "## üìå Project Overview\n",
        "- **Task**: Unpaired image-to-image translation (Selfie ‚Üî Anime)\n",
        "- **Dataset**: Selfie2Anime dataset from Kaggle\n",
        "- **Architecture**: CycleGAN with Generator and Discriminator networks\n",
        "- **Goal**: Generate realistic anime-style images from selfies and vice versa\n",
        "\n",
        "## üìö Learning Objectives\n",
        "By completing this assignment, you will:\n",
        "- Understand adversarial training and GAN architectures\n",
        "- Implement cycle consistency and identity loss functions\n",
        "- Build Generator and Discriminator networks\n",
        "- Train unpaired image translation models\n",
        "- Evaluate generative model performance\n",
        "\n",
        "![CycleGAN Process](https://www.oreilly.com/api/v2/epubs/9781788836067/files/assets/d5036aa6-77cf-41c1-8828-11436977198e.png)\n",
        "\n",
        "[Image Source](https://www.oreilly.com/library/view/hands-on-artificial-intelligence/9781788836067/c2e7d914-4e45-4528-8627-c590d19107ef.xhtml)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmSL6WHfdUHP"
      },
      "source": [
        "## 1Ô∏è‚É£ Import Libraries and Configuration\n",
        "\n",
        "**Task**: Import all necessary libraries and set up configuration parameters.\n",
        "\n",
        "**Requirements**:\n",
        "- Import PyTorch, torchvision, and GAN-specific libraries\n",
        "- Import visualization and utility libraries\n",
        "- Set random seeds for reproducibility\n",
        "- Configure training hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H-h3d6MaG0B"
      },
      "outputs": [],
      "source": [
        "# TODO: Import core PyTorch libraries\n",
        "# torch, torch.nn, torch.optim\n",
        "# torchvision.transforms, torchvision.utils\n",
        "\n",
        "# TODO: Import dataset and data handling\n",
        "# torch.utils.data (DataLoader, Dataset)\n",
        "# kagglehub for dataset download\n",
        "\n",
        "# TODO: Import visualization and utilities\n",
        "# matplotlib.pyplot, numpy, PIL.Image\n",
        "# itertools, tqdm, random, os\n",
        "\n",
        "# TODO: Set random seeds for reproducibility (use seed=42)\n",
        "# torch.manual_seed, np.random.seed, random.seed\n",
        "\n",
        "# TODO: Check device availability and print\n",
        "\n",
        "# TODO: Define configuration parameters:\n",
        "IMG_SIZE = 64           # Image resolution (64x64)\n",
        "BATCH_SIZE = 4          # Batch size (small due to memory constraints)\n",
        "LEARNING_RATE = 0.0002  # Learning rate for both generators and discriminators\n",
        "BETA1 = 0.5             # Beta1 for Adam optimizer\n",
        "BETA2 = 0.999           # Beta2 for Adam optimizer\n",
        "NUM_EPOCHS = 15         # Number of training epochs\n",
        "LAMBDA_CYCLE = 10.0     # Weight for cycle consistency loss\n",
        "LAMBDA_IDENTITY = 5.0   # Weight for identity loss\n",
        "N_RESIDUAL_BLOCKS = 6   # Number of residual blocks in generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cenxNS1KdWNG"
      },
      "source": [
        "## 2Ô∏è‚É£ Dataset Download and Loading\n",
        "\n",
        "**Task**: Download the Selfie2Anime dataset and create data loaders.\n",
        "\n",
        "**Requirements**:\n",
        "- Download dataset using kagglehub\n",
        "- Create custom dataset class for unpaired data\n",
        "- Apply appropriate transformations (resize, normalize)\n",
        "- Create train and test data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRB1sbj1dVEU"
      },
      "outputs": [],
      "source": [
        "# TODO: Download the Selfie2Anime dataset\n",
        "# path = kagglehub.dataset_download(\"arnaud58/selfie2anime\")\n",
        "# print(\"Path to dataset files:\", path)\n",
        "\n",
        "# TODO: Define image transformations:\n",
        "# Use transforms.Compose with:\n",
        "# - Resize to (IMG_SIZE, IMG_SIZE)\n",
        "# - ToTensor()\n",
        "# - Normalize with mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5) for [-1,1] range\n",
        "\n",
        "# TODO: Set up dataset paths:\n",
        "# train_selfie_path = os.path.join(dataset_path, \"trainA\")  # Selfie images\n",
        "# train_anime_path = os.path.join(dataset_path, \"trainB\")   # Anime images\n",
        "# test_selfie_path = os.path.join(dataset_path, \"testA\")    # Test selfie images\n",
        "# test_anime_path = os.path.join(dataset_path, \"testB\")     # Test anime images\n",
        "\n",
        "# TODO: Create custom dataset class (SelfieAnimeDataset)\n",
        "# Requirements:\n",
        "# - __init__(self, root_selfie, root_anime, transform=None)\n",
        "# - Load image file lists from both domains\n",
        "# - __len__ returns max length of both domains\n",
        "# - __getitem__ returns (selfie_img, anime_img) using modulo for cycling\n",
        "\n",
        "# TODO: Create train and test datasets\n",
        "# TODO: Create data loaders with appropriate batch size and shuffle\n",
        "# TODO: Print dataset sizes and verify data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iTkcFoIdYHy"
      },
      "source": [
        "## 3Ô∏è‚É£ Generator Architecture\n",
        "\n",
        "**Task**: Implement the Generator network with residual blocks.\n",
        "\n",
        "**Requirements**:\n",
        "- Create ResidualBlock class with skip connections\n",
        "- Implement Generator with encoder-decoder structure\n",
        "- Use reflection padding and instance normalization\n",
        "- Include downsampling, residual blocks, and upsampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdhBpBLDdZam"
      },
      "outputs": [],
      "source": [
        "# TODO: Create ResidualBlock class:\n",
        "# - Inherit from nn.Module\n",
        "# - Use reflection padding, conv2d, instance norm, ReLU\n",
        "# - Implement skip connection in forward pass: return x + conv_block(x)\n",
        "\n",
        "# TODO: Create Generator class:\n",
        "# In __init__:\n",
        "# 1. Initial convolution block:\n",
        "#    - ReflectionPad2d(3), Conv2d(3->64, kernel=7), InstanceNorm2d, ReLU\n",
        "#\n",
        "# 2. Downsampling layers (2 layers):\n",
        "#    - Conv2d with stride=2, InstanceNorm2d, ReLU\n",
        "#    - Features: 64 -> 128 -> 256\n",
        "#\n",
        "# 3. Residual blocks:\n",
        "#    - Add N_RESIDUAL_BLOCKS ResidualBlock layers\n",
        "#\n",
        "# 4. Upsampling layers (2 layers):\n",
        "#    - ConvTranspose2d with stride=2, InstanceNorm2d, ReLU\n",
        "#    - Features: 256 -> 128 -> 64\n",
        "#\n",
        "# 5. Output layer:\n",
        "#    - ReflectionPad2d(3), Conv2d(64->3, kernel=7), Tanh()\n",
        "\n",
        "# TODO: Implement forward method\n",
        "# TODO: Test generator with random input to verify output shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l6ZxTHHdafM"
      },
      "source": [
        "## 4Ô∏è‚É£ Discriminator Architecture\n",
        "\n",
        "**Task**: Implement the Discriminator network for adversarial training.\n",
        "\n",
        "**Requirements**:\n",
        "- Create PatchGAN discriminator architecture\n",
        "- Use leaky ReLU activations and instance normalization\n",
        "- Output patch-based predictions rather than single value\n",
        "- Handle both real and fake image discrimination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxtnJJMSdba-"
      },
      "outputs": [],
      "source": [
        "# TODO: Create Discriminator class:\n",
        "# Create helper function discriminator_block(in_filters, out_filters, normalize=True):\n",
        "# - Conv2d(kernel=4, stride=2, padding=1)\n",
        "# - InstanceNorm2d (if normalize=True)\n",
        "# - LeakyReLU(0.2)\n",
        "\n",
        "# TODO: In __init__:\n",
        "# Build discriminator using discriminator_block:\n",
        "# - Layer 1: 3 -> 64 (no normalization)\n",
        "# - Layer 2: 64 -> 128\n",
        "# - Layer 3: 128 -> 256\n",
        "# - Layer 4: 256 -> 512\n",
        "# - Final: ZeroPad2d + Conv2d(512->1, kernel=4)\n",
        "\n",
        "# TODO: Implement forward method\n",
        "# TODO: Test discriminator with random input to verify output shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gszv3QfPdcYl"
      },
      "source": [
        "## 5Ô∏è‚É£ Loss Functions\n",
        "\n",
        "**Task**: Implement the three types of losses used in CycleGAN.\n",
        "\n",
        "**Requirements**:\n",
        "- Adversarial loss for generator and discriminator training\n",
        "- Cycle consistency loss to ensure cycle A‚ÜíB‚ÜíA ‚âà A\n",
        "- Identity loss to preserve color composition\n",
        "- Combine losses with appropriate weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L3zJJmVdc-6"
      },
      "outputs": [],
      "source": [
        "# TODO: Create CycleGANLoss class:\n",
        "# In __init__:\n",
        "# - Store lambda_cyc and lambda_id weights\n",
        "# - Initialize MSELoss for adversarial loss\n",
        "# - Initialize L1Loss for cycle and identity losses\n",
        "\n",
        "# TODO: Implement adversarial_loss(self, pred, target_is_real):\n",
        "# - Create target tensor (ones for real, zeros for fake)\n",
        "# - Return MSE loss between prediction and target\n",
        "\n",
        "# TODO: Implement cycle_consistency_loss(self, real_images, cycled_images):\n",
        "# - Return L1 loss between original and cycle-reconstructed images\n",
        "\n",
        "# TODO: Implement identity_loss(self, real_images, same_images):\n",
        "# - Return L1 loss between real images and same-domain generated images\n",
        "# - Used when G_AB(B) should ‚âà B and G_BA(A) should ‚âà A\n",
        "\n",
        "# TODO: Initialize loss function with configured weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMNsgPnrdd3U"
      },
      "source": [
        "## 6Ô∏è‚É£ Model Initialization and Optimizers\n",
        "\n",
        "**Task**: Initialize all models and optimizers for CycleGAN training.\n",
        "\n",
        "**Requirements**:\n",
        "- Create two generators: G_AB (Selfie‚ÜíAnime) and G_BA (Anime‚ÜíSelfie)\n",
        "- Create two discriminators: D_A (for Selfie domain) and D_B (for Anime domain)\n",
        "- Initialize optimizers for generators and discriminators separately\n",
        "- Move all models to appropriate device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mpdU2XGdet9"
      },
      "outputs": [],
      "source": [
        "# TODO: Initialize models and move to device:\n",
        "# G_AB = Generator().to(device)  # Selfie to Anime\n",
        "# G_BA = Generator().to(device)  # Anime to Selfie\n",
        "# D_A = Discriminator().to(device)  # Discriminator for Selfie domain\n",
        "# D_B = Discriminator().to(device)  # Discriminator for Anime domain\n",
        "\n",
        "# TODO: Initialize optimizers:\n",
        "# optimizer_G: Adam optimizer for both generators (itertools.chain)\n",
        "# optimizer_D_A: Adam optimizer for discriminator A\n",
        "# optimizer_D_B: Adam optimizer for discriminator B\n",
        "# Use lr=LEARNING_RATE, betas=(BETA1, BETA2)\n",
        "\n",
        "# TODO: Initialize loss function with configured weights\n",
        "# TODO: Print model architectures and parameter counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ilSVW1NdfeO"
      },
      "source": [
        "## 7Ô∏è‚É£ Training Function\n",
        "\n",
        "**Task**: Implement the CycleGAN training loop for one epoch.\n",
        "\n",
        "**Requirements**:\n",
        "- Train generators with adversarial, cycle, and identity losses\n",
        "- Train discriminators to distinguish real from fake images\n",
        "- Alternate between generator and discriminator updates\n",
        "- Track and return loss values for monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSQLwWEVdgaB"
      },
      "outputs": [],
      "source": [
        "# TODO: Create train_epoch(epoch) function:\n",
        "\n",
        "# TODO: Set all models to training mode\n",
        "# TODO: Initialize running loss trackers\n",
        "\n",
        "# TODO: For each batch (real_A, real_B):\n",
        "#\n",
        "# === TRAIN GENERATORS ===\n",
        "# 1. Zero generator gradients\n",
        "#\n",
        "# 2. Identity Loss:\n",
        "#    - loss_id_A = criterion.identity_loss(real_A, G_BA(real_A))\n",
        "#    - loss_id_B = criterion.identity_loss(real_B, G_AB(real_B))\n",
        "#    - loss_identity = (loss_id_A + loss_id_B) / 2\n",
        "#\n",
        "# 3. Adversarial Loss:\n",
        "#    - fake_B = G_AB(real_A)\n",
        "#    - loss_GAN_AB = criterion.adversarial_loss(D_B(fake_B), True)\n",
        "#    - fake_A = G_BA(real_B)\n",
        "#    - loss_GAN_BA = criterion.adversarial_loss(D_A(fake_A), True)\n",
        "#    - loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "#\n",
        "# 4. Cycle Consistency Loss:\n",
        "#    - recovered_A = G_BA(fake_B)\n",
        "#    - loss_cycle_ABA = criterion.cycle_consistency_loss(real_A, recovered_A)\n",
        "#    - recovered_B = G_AB(fake_A)\n",
        "#    - loss_cycle_BAB = criterion.cycle_consistency_loss(real_B, recovered_B)\n",
        "#    - loss_cycle = (loss_cycle_ABA + loss_cycle_BAB) / 2\n",
        "#\n",
        "# 5. Total Generator Loss:\n",
        "#    - loss_G = loss_GAN + LAMBDA_CYCLE * loss_cycle + LAMBDA_IDENTITY * loss_identity\n",
        "#    - loss_G.backward() and optimizer_G.step()\n",
        "#\n",
        "# === TRAIN DISCRIMINATOR A ===\n",
        "# 1. Zero discriminator A gradients\n",
        "# 2. Real loss: loss_real_A = criterion.adversarial_loss(D_A(real_A), True)\n",
        "# 3. Fake loss: loss_fake_A = criterion.adversarial_loss(D_A(fake_A.detach()), False)\n",
        "# 4. Total: loss_D_A = (loss_real_A + loss_fake_A) / 2\n",
        "# 5. Backward and step\n",
        "#\n",
        "# === TRAIN DISCRIMINATOR B ===\n",
        "# Similar to Discriminator A but for domain B\n",
        "#\n",
        "# TODO: Track running losses and return epoch averages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWigJJwpdheS"
      },
      "source": [
        "## 8Ô∏è‚É£ Training Loop and Monitoring\n",
        "\n",
        "**Task**: Execute the full training process with progress monitoring.\n",
        "\n",
        "**Requirements**:\n",
        "- Train for specified number of epochs\n",
        "- Display loss values and training progress\n",
        "- Save sample images during training for visual monitoring\n",
        "- Track loss curves for analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNNeVrRfdiXm"
      },
      "outputs": [],
      "source": [
        "# TODO: Initialize loss tracking lists\n",
        "# g_losses = []\n",
        "# d_losses = []\n",
        "\n",
        "# TODO: Training loop:\n",
        "# for epoch in range(NUM_EPOCHS):\n",
        "#     g_loss, d_loss = train_epoch(epoch)\n",
        "#     g_losses.append(g_loss)\n",
        "#     d_losses.append(d_loss)\n",
        "#\n",
        "#     print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] - G_loss: {g_loss:.4f}, D_loss: {d_loss:.4f}\")\n",
        "#\n",
        "#     # Save sample images every 5 epochs\n",
        "#     if (epoch + 1) % 5 == 0:\n",
        "#         TODO: Generate and save sample translations\n",
        "#         - Set models to eval mode\n",
        "#         - Get test batch\n",
        "#         - Generate fake_B = G_AB(real_A) and fake_A = G_BA(real_B)\n",
        "#         - Create comparison grid and save\n",
        "\n",
        "# TODO: Plot training loss curves\n",
        "# Create matplotlib plot showing generator and discriminator losses over epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQStTz56djSi"
      },
      "source": [
        "## 9Ô∏è‚É£ Test Set Evaluation\n",
        "\n",
        "**Task**: Evaluate the trained model on test data with comprehensive visualization.\n",
        "\n",
        "**Requirements**:\n",
        "- Generate 10 selfie-to-anime translations\n",
        "- Generate 10 anime-to-selfie translations  \n",
        "- Display results in organized grid format\n",
        "- Show original and generated images side by side"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2SRWTcPdkB4"
      },
      "outputs": [],
      "source": [
        "# TODO: Create evaluate_on_test_set() function:\n",
        "# 1. Set models to evaluation mode\n",
        "# 2. Use torch.no_grad() for inference\n",
        "# 3. Collect 10 test images from each domain\n",
        "# 4. Generate translations using both generators\n",
        "# 5. Create 4x10 subplot grid:\n",
        "#    - Row 1: Original selfies\n",
        "#    - Row 2: Generated anime (from selfies)\n",
        "#    - Row 3: Original anime\n",
        "#    - Row 4: Generated selfies (from anime)\n",
        "# 6. Display with appropriate titles and no axes\n",
        "\n",
        "# TODO: Call evaluation function and display results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spDCs3nSdlZa"
      },
      "source": [
        "## üîü Internet Images Evaluation\n",
        "\n",
        "**Task**: Test the model on external images from the internet.\n",
        "\n",
        "**Requirements**:\n",
        "- Load 3 selfie images and 3 anime images from provided URLs\n",
        "- Apply same preprocessing as training data\n",
        "- Generate translations in both directions\n",
        "- Display results to demonstrate generalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6ZN8YAVdmIy"
      },
      "outputs": [],
      "source": [
        "# TODO: Create load_internet_image_from_url(url) function:\n",
        "# 1. Use requests to download image with appropriate headers\n",
        "# 2. Handle errors and content type validation\n",
        "# 3. Apply same transforms as training (resize, normalize)\n",
        "# 4. Return processed tensor or None if failed\n",
        "\n",
        "# TODO: Create evaluate_internet_images() function:\n",
        "# 1. Define image URLs for selfies and anime\n",
        "# Some images you can use, you can use your own.\n",
        "# selfie_urls = [\n",
        "#     'https://i.imgur.com/m7Em3S2.png',\n",
        "#     'https://i.imgur.com/2lzy9Un.png',\n",
        "#     'https://i.imgur.com/eg7lsZ2.png'\n",
        "# ]\n",
        "# anime_urls = [\n",
        "#     'https://i.imgur.com/2lCFEIY.png',\n",
        "#     'https://i.imgur.com/kYXGdqM.png',\n",
        "#     'https://i.imgur.com/G6tQMt9.png'\n",
        "# ]\n",
        "#\n",
        "# 2. Load images and generate translations\n",
        "# 3. Create 2x6 subplot showing:\n",
        "#    - Row 1: Internet selfie, Generated anime (repeat for 3 pairs)\n",
        "#    - Row 2: Internet anime, Generated selfie (repeat for 3 pairs)\n",
        "\n",
        "# TODO: Call evaluation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEkgHwW7drnO"
      },
      "source": [
        "## 1Ô∏è‚É£1Ô∏è‚É£ Model Saving and Analysis\n",
        "\n",
        "**Task**: Save trained models and analyze the learning process.\n",
        "\n",
        "**Requirements**:\n",
        "- Save all trained model state dictionaries\n",
        "- Analyze training stability and convergence\n",
        "- Discuss quality of generated images\n",
        "- Document observations and potential improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4I68vR-7dtA_"
      },
      "outputs": [],
      "source": [
        "# TODO: Save all trained models:\n",
        "# torch.save(G_AB.state_dict(), 'generator_AB.pth')\n",
        "# torch.save(G_BA.state_dict(), 'generator_BA.pth')\n",
        "# torch.save(D_A.state_dict(), 'discriminator_A.pth')\n",
        "# torch.save(D_B.state_dict(), 'discriminator_B.pth')\n",
        "\n",
        "# TODO: Analyze training results:\n",
        "# 1. Comment on loss convergence and stability\n",
        "# 2. Evaluate quality of selfie-to-anime translations\n",
        "# 3. Evaluate quality of anime-to-selfie translations\n",
        "# 4. Discuss which direction works better and why\n",
        "# 5. Identify potential improvements (longer training, different architectures, etc.)\n",
        "\n",
        "print(\"Training completed and models saved!\")\n",
        "print(\"Analysis: [Write your observations about the training process and results]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMg_3CbLduh7"
      },
      "source": [
        "## üìù Evaluation Criteria\n",
        "\n",
        "Your homework will be evaluated based on:\n",
        "\n",
        "1. **Implementation Correctness (40%)**\n",
        "   - Proper Generator and Discriminator architectures\n",
        "   - Correct loss function implementations (adversarial, cycle, identity)\n",
        "   - Working training loop with proper gradient updates\n",
        "   - Appropriate data loading and preprocessing\n",
        "\n",
        "2. **Model Performance (30%)**\n",
        "   - Model trains without errors for full epoch count\n",
        "   - Generated images show clear style transfer\n",
        "   - Loss curves demonstrate learning progress\n",
        "   - Reasonable visual quality in translations\n",
        "\n",
        "3. **Code Quality (30%)**\n",
        "   - Clean, readable code with proper comments\n",
        "   - Efficient tensor operations and device management\n",
        "   - Proper use of PyTorch best practices\n",
        "   - Well-structured class definitions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
